# Repositorio Procesamiento del Lenguaje Natural (NLP) 
## Autor: Pablo Segovia

# Resumen
El presente repositorio describe los trabajos realizados sobre diferentes temas de Procesamiento del Lenguaje Natural (NLP).

1. Similitud con vectorizaci贸n de texto [К](https://github.com/manoloacademia/CEIA_NLP/blob/main/Desafio_1_PabloSegovia.ipynb)
2. Generaci贸n de embeddings sobre Cr铆men y Castigo [锔](https://github.com/manoloacademia/CEIA_NLP/blob/main/Desafio_2_PabloSegovia.ipynb)
3. Modelos de lenguaje basados en arquitecturas recurrentes [](https://github.com/manoloacademia/CEIA_NLP/blob/main/Desaf%C3%ADo%203_PabloSegovia.ipynb)
4. Chatbot Q&A con LSTM [](https://github.com/manoloacademia/CEIA_NLP/blob/main/Desaf%C3%ADo_4_PabloSegovia.ipynb)
5. Clasificador con BERT como encoder [猸](https://github.com/manoloacademia/NLP/blob/main/Desaf铆o_5_PabloSegovia.ipynb)

# Detalle de cada trabajo

## Trabajo 1: similitud con vectorizaci贸n de texto
<img src="tf-idf.JPG" alt="drawing" width="600"/>

- El [Trabajo 1](https://github.com/manoloacademia/CEIA_NLP/blob/main/Desafio_1_PabloSegovia.ipynb) soluciona un problema de similitud realizando una vectorizaci贸n de texto y modelo de clasificaci贸n Na茂ve Bayes con el dataset [20 newsgroups](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html).
- Para poder resolverlo, se utiliz贸 un vectorizador del tipo [``TfidfVectorizer()``](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) para hacer la extracci贸n de features.
- Se estudi贸 la similaridad de los documentos utilizando la similaridad del coseno como medida de la similitud existente entre dos vectores en un espacio.
- Se entrenaron modelos de clasificaci贸n Na茂ve Bayes Multinomial y ComplementNB para maximizar el desempe帽o de clasificaci贸n (f1-score macro).
- Se estudi贸 tambi茅n transponer la matriz documento-t茅rmino, para obtener una matriz t茅rmino-documento que puede ser interpretada como una colecci贸n de vectorizaci贸n de palabras.
- Se encontr贸 que con el n煤mero de features original (vocabulario 101631) muchas de las palabras dentro del diccionario son palabras que no forman parte del vocabulario Ingl茅s pers茅, sino palabras que quedaron dentro del vocabulario del texto como "malos tokens".
- Se encontr贸 que incluso bajando el n煤mero de features tenemos estas palabras pero en menor proporci贸n.
- Al final yendo con 10.000 palabras, si bien el resultado de las similares son palabras del ingl茅s, no existe mucha similaridad contextual entre ellas.

## Trabajo 2: generaci贸n de embeddings sobre Cr铆men y Castigo
<img src="w2v_2D.PNG" alt="drawing" width="800"/>

- El [Trabajo 2](https://github.com/manoloacademia/CEIA_NLP/blob/main/Desafio_2_PabloSegovia.ipynb) es utilizar documentos / corpus para crear embeddings de palabras basado en ese contexto.
- Se utiliz贸 el libro [Crimen y Castigo de Fi贸dor Dostoievski](https://www.gutenberg.org/ebooks/61851) para generar los embeddings, es decir, que los vectores tendr谩n la forma en funci贸n de como ese libro haya sido escrito.
- Generamos nuestros propios vectores de embedding [Gensim](https://www.machinelearningplus.com/nlp/gensim-tutorial/).
- Se entren贸 un modelo Word2Vec utilizando la estructura modelo Skipgram.
- Se trat贸 de explicar los personajes de la novela buscando las palabras similares con embeddings m谩s cercanos. Dejo la fuente de referencia de la novela en este [link](https://es.wikipedia.org/wiki/Crimen_y_castigo#Personajes).
- Se trat贸 tambi茅n de explicar las partes del libro por las palabras similares de la idea general de cada parte.
- Podemos decir que usando este m茅todo Word2Vec, los embeddings de los personajes tienen relaci贸n sem谩ntica con las palabras similares.
- Al poder graficarlos, vemos que reduciendo dimensionalidad a 2D, tenemos la posibilidad de ubicar tanto nombre y apellido de un personaje como puntos cercanos del plano.
- Podemos agregar que buscando tan solo 1 palabra en cada una de las partes del libros, se puede apreciar la similitud en varias palabras con el contexto de lo que realmente pasa en el libro. As铆, la similitud sem谩ntica es aparente.
- Al buscar el t铆tulo del libro dentro del texto en cuesti贸n, vemos que tambi茅n hay mucha similitud con las palabras que son ajenas al text. Estas palabras forman parte del detalle de propaganda y descripci贸n del Proyecto Gutenberg. De esta forma, se abre una opotunidad para chequear y "filtrar" este tipo de palabras del corpus.

## Trabajo 3: modelos de lenguaje basados en arquitecturas recurrentes
<img src="RNN.PNG" alt="drawing" width="800"/>

- El [Trabajo 3](https://github.com/manoloacademia/CEIA_NLP/blob/main/Desaf%C3%ADo%203_PabloSegovia.ipynb) describe entrenar un modelo de lenguaje basado en arquitectura recurrente.
- Se utiliza el corpus [A Million News Headlines](https://www.kaggle.com/datasets/therohk/million-headlines/data) que contiene la data de encabezados de noticias de la ABC Australiana en los 煤ltimos 19 a帽os.
- El objetivo principal ser铆a lograr que durante el entrenamiento baje la [perplejidad](https://medium.com/nlplanet/two-minutes-nlp-perplexity-explained-with-simple-probabilities-6cdc46884584). Este concepto permite comparar modelos.
- Se exploraron diferentes modelos buscando este objetivo: LSTM, Conv1D y Bidirectional LSTM, Bidirectional LSTM stack, Conv1D stack, Bidirectional LSTM y GRU, Aumento de neuronas, Cambio de size del contexto m谩ximo y Cambio del tama帽o del corpus.
- Si bien nunca se pudo generar durante el entrenamiento que la perplejidad sea menor luego de cada 茅poca, se tiene un valor bajo al ir puliendo la arquitectura del modelo.
- Se realiza una generaci贸n de secuencias random y usando beam search con buen resultado.

## Trabajo 4: chatbot Q&A con LSTM
<img src="bot.PNG" alt="drawing" width="800"/>

- El [Trabajo 4](https://github.com/manoloacademia/CEIA_NLP/blob/main/Desaf%C3%ADo_4_PabloSegovia.ipynb) describe realizar un Bot de Q&A usando un modelo LSTM.
- Se basa en un dataset del challenge ConvAI2 [Conversational Intelligence Challenge 2](https://convai.io/data/) y en 茅l tratamos de responder las preguntas del usuario.
- Se realiza un preprocesamiento necesario para obtener los diccionarios de 铆ndices para los tokens, tanto entrada como salida.
- Se utilizaron los embedding de [Glove](https://nlp.stanford.edu/projects/glove/) para transformar los tokens de entrada en vectores.
- Se entrena un modelo basado en el esquema encoder-decoder utilizando los datos generados.
- Se genera inferencia del modelo y se obtiene un buen nivel de coherencia en las respuestas, aunque no se pueda generar una respuesta directa.
- Para poder mejorar el modelo har铆a falta poder consumir todo el dataset y todo el vocabulario, pero la cantidad de RAM no es suficiente con los medios disponibles gratuitos y locales.

## Trabajo 5: clasificaci贸n con BERT como encoder
<img src="bert.PNG" alt="drawing" width="800"/>

- El [Trabajo 5](https://github.com/manoloacademia/NLP/blob/main/Desaf铆o_5_PabloSegovia.ipynb) describe entrenar un modelo ed clasificaci贸n de [Sentiment Analysis](https://huggingface.co/blog/sentiment-analysis-python) para un dataset de reviews de apps utilizando el modelo [BERT](https://huggingface.co/docs/transformers/model_doc/bert) como encoder.
- 


## Contacto
Pueden escribirme directamente a mi direcci贸n de mail:  luispablosegovia@icloud.com 

O tal vez directamente a mi LinkedIn [](https://www.linkedin.com/in/luispablosegovia/)

Y pasear por mis repositorios en Github [:octocat:](https://github.com/manoloacademia?tab=repositories)

Saludos!

### Licencia: MIT
