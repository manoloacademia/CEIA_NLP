{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorización de texto y modelo de clasificación Naïve Bayes con el dataset 20 newsgroups"
      ],
      "metadata": {
        "id": "zq6j8LsYq1Dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# 20newsgroups por ser un dataset clásico de NLP ya viene incluido y formateado\n",
        "# en sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "l7cXR6CI30ry"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga de datos"
      ],
      "metadata": {
        "id": "yD-pVDWV_rQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cargamos los datos (ya separados de forma predeterminada en train y test)\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
      ],
      "metadata": {
        "id": "Ech9qJaUo9vK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorización"
      ],
      "metadata": {
        "id": "UxjSI7su_uWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instanciamos un vectorizador\n",
        "# ver diferentes parámetros de instanciación en la documentación de sklearn\n",
        "tfidfvect = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "-94VP0QYCzDn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# en el atributo `data` accedemos al texto\n",
        "newsgroups_train.data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "ftPlyanuak8n",
        "outputId": "70f26f39-6a22-4950-aa35-e2e10ec62b88"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# con la interfaz habitual de sklearn podemos fitear el vectorizador\n",
        "# (obtener el vocabulario y calcular el vector IDF)\n",
        "# y transformar directamente los datos\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
        "# `X_train` la podemos denominar como la matriz documento-término"
      ],
      "metadata": {
        "id": "1zxcXV6aC_oL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recordar que las vectorizaciones por conteos son esparsas\n",
        "# por ello sklearn convenientemente devuelve los vectores de documentos\n",
        "# como matrices esparsas\n",
        "print(type(X_train))\n",
        "print(f'shape: {X_train.shape}')\n",
        "print(f'cantidad de documentos: {X_train.shape[0]}')\n",
        "print(f'tamaño del vocabulario (dimensionalidad de los vectores): {X_train.shape[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Sv7TXbda41-",
        "outputId": "42da2365-c151-4610-c84c-893a1e48cb7c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'scipy.sparse._csr.csr_matrix'>\n",
            "shape: (11314, 101631)\n",
            "cantidad de documentos: 11314\n",
            "tamaño del vocabulario (dimensionalidad de los vectores): 101631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# una vez fiteado el vectorizador, podemos acceder a atributos como el vocabulario\n",
        "# aprendido. Es un diccionario que va de términos a índices.\n",
        "# El índice es la posición en el vector de documento.\n",
        "tfidfvect.vocabulary_['car']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgydNTZ2pAgR",
        "outputId": "8618ba67-ef60-4e74-d244-e4f6dcfdaeab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25775"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# es muy útil tener el diccionario opuesto que va de índices a términos\n",
        "idx2word = {v: k for k,v in tfidfvect.vocabulary_.items()}"
      ],
      "metadata": {
        "id": "xnTSZuvyrTcP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# en `y_train` guardamos los targets que son enteros\n",
        "y_train = newsgroups_train.target\n",
        "y_train[:10]"
      ],
      "metadata": {
        "id": "swa-AgWrMSHM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1df3f5d2-1444-4b5c-c6ef-6b033c37993f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hay 20 clases correspondientes a los 20 grupos de noticias\n",
        "print(f'clases {np.unique(newsgroups_test.target)}')\n",
        "newsgroups_test.target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je5kxvQMDLvf",
        "outputId": "e5fa327b-690f-4b86-c5ff-4b4080e7eefc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clases [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similaridad de documentos"
      ],
      "metadata": {
        "id": "SXCICFSd_y90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos similaridad de documentos. Tomemos algún documento\n",
        "idx = 4811\n",
        "print(newsgroups_train.data[idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pki_olShnyE",
        "outputId": "b2b9feda-af24-473b-a94e-cec7b0c389a5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE WHITE HOUSE\n",
            "\n",
            "                  Office of the Press Secretary\n",
            "                   (Pittsburgh, Pennslyvania)\n",
            "______________________________________________________________\n",
            "For Immediate Release                         April 17, 1993     \n",
            "\n",
            "             \n",
            "                  RADIO ADDRESS TO THE NATION \n",
            "                        BY THE PRESIDENT\n",
            "             \n",
            "                Pittsburgh International Airport\n",
            "                    Pittsburgh, Pennsylvania\n",
            "             \n",
            "             \n",
            "10:06 A.M. EDT\n",
            "             \n",
            "             \n",
            "             THE PRESIDENT:  Good morning.  My voice is coming to\n",
            "you this morning through the facilities of the oldest radio\n",
            "station in America, KDKA in Pittsburgh.  I'm visiting the city to\n",
            "meet personally with citizens here to discuss my plans for jobs,\n",
            "health care and the economy.  But I wanted first to do my weekly\n",
            "broadcast with the American people. \n",
            "             \n",
            "             I'm told this station first broadcast in 1920 when\n",
            "it reported that year's presidential elections.  Over the past\n",
            "seven decades presidents have found ways to keep in touch with\n",
            "the people, from whistle-stop tours to fire-side chats to the bus\n",
            "tour that I adopted, along with Vice President Gore, in last\n",
            "year's campaign.\n",
            "             \n",
            "             Every Saturday morning I take this time to talk with\n",
            "you, my fellow Americans, about the problems on your minds and\n",
            "what I'm doing to try and solve them.  It's my way of reporting\n",
            "to you and of giving you a way to hold me accountable.\n",
            "             \n",
            "             You sent me to Washington to get our government and\n",
            "economy moving after years of paralysis and policy and a bad\n",
            "experiment with trickle-down economics.  You know how important\n",
            "it is for us to make bold, comprehensive changes in the way we do\n",
            "business.  \n",
            "             \n",
            "             We live in a competitive global economy.  Nations\n",
            "rise and fall on the skills of their workers, the competitiveness\n",
            "of their companies, the imagination of their industries, and the\n",
            "cooperative experience and spirit that exists between business,\n",
            "labor and government.  Although many of the economies of the\n",
            "industrialized world are now suffering from slow growth, they've\n",
            "made many of the smart investments and the tough choices which\n",
            "our government has for too long ignored.  That's why many of them\n",
            "have been moving ahead and too many of our people have been\n",
            "falling behind.\n",
            "             \n",
            "             We have an economy today that even when it grows is\n",
            "not producing new jobs.  We've increased the debt of our nation\n",
            "by four times over the last 12 years, and we don't have much to\n",
            "show for it.  We know that wages of most working people have\n",
            "stopped rising, that most people are working longer work weeks\n",
            "and that too many families can no longer afford the escalating\n",
            "cost of health care.\n",
            "             \n",
            "             But we also know that, given the right tools, the\n",
            "right incentives and the right encouragement, our workers and\n",
            "businesses can make the kinds of products and profits our economy\n",
            "needs to expand opportunity and to make our communities better\n",
            "places to live.\n",
            "             \n",
            "             In many critical products today Americans are the\n",
            "low cost, high quality producers.  Our task is to make sure that\n",
            "we create more of those kinds of jobs.\n",
            "             \n",
            "             Just two months ago I gave Congress my plan for\n",
            "long-term jobs and economic growth.  It changes the old\n",
            "priorities in Washington and puts our emphasis where it needs to\n",
            "be -- on people's real needs, on increasing investments and jobs\n",
            "and education, on cutting the federal deficit, on stopping the\n",
            "waste which pays no dividends, and redirecting our precious\n",
            "resources toward investment that creates jobs now and lays the\n",
            "groundwork for robust economic growth in the future.\n",
            "             \n",
            "             These new directions passed the Congress in record\n",
            "time and created a new sense of hope and opportunity in our\n",
            "country.  Then the jobs plan I presented to Congress, which would\n",
            "create hundreds of thousands of jobs, most of them in the private\n",
            "sector in 1993 and 1994, passed the House of Representatives.  It\n",
            "now has the support of a majority of the United States Senate. \n",
            "But it's been held up by a filibuster of a minority in the\n",
            "Senate, just 43 senators.  They blocked a vote that they know\n",
            "would result in the passage of our bill and the creation of jobs.\n",
            "             \n",
            "             The issue isn't politics; the issue is people. \n",
            "Millions of Americans are waiting for this legislation and\n",
            "counting on it, counting on us in Washington.  But the jobs bill\n",
            "has been grounded by gridlock.  \n",
            "             \n",
            "             I know the American people are tired of business as\n",
            "usual and politics as usual.  I know they don't want us to spin\n",
            "or wheels.  They want the recovery to get moving.  So I have\n",
            "taken a first step to break this gridlock and gone the extra\n",
            "mile.  Yesterday I offered to cut the size of this plan by 25\n",
            "percent -- from $16 billion to $12 billion.  \n",
            "             \n",
            "             It's not what I'd hoped for.  With 16 million\n",
            "Americans looking for full-time work, I simply can't let the bill\n",
            "languish when I know that even a compromise bill will mean\n",
            "hundreds of thousands of jobs for our people.  The mandate is to\n",
            "act to achieve change and move the country forward.  By taking\n",
            "this initiative in the face of an unrelenting Senate talkathon, I\n",
            "think we can respond to your mandate and achieve a significant\n",
            "portion of our original goals.\n",
            "             \n",
            "             First, we want to keep the programs as much as\n",
            "possible that are needed to generate jobs and meet human needs,\n",
            "including highway and road construction, summer jobs for young\n",
            "people, immunization for children, construction of waste water\n",
            "sites, and aid to small businesses.  We also want to keep funding\n",
            "for extended unemployment compensation benefits, for people who\n",
            "have been unemployed for a long time because the economy isn't\n",
            "creating jobs.\n",
            "             \n",
            "             Second, I've recommended that all the other programs\n",
            "in the bill be cut across-the-board by a little more than 40\n",
            "percent.\n",
            "             \n",
            "             And third, I've recommended a new element in this\n",
            "program to help us immediately start our attempt to fight against\n",
            "crime by providing $200 million for cities and towns to rehire\n",
            "police officers who lost their jobs during the recession and put\n",
            "them back to work protecting our people.  I'm also going to fight\n",
            "for a tough crime bill because the people of this country need it\n",
            "and deserve it.\n",
            "             \n",
            "             Now, the people who are filibustering this bill --\n",
            "the Republican senators -- say they won't vote for it because it\n",
            "increases deficit spending, because there's extra spending this\n",
            "year that hasn't already been approved.  That sounds reasonable,\n",
            "doesn't it?  Here's what they don't say.  This program is more\n",
            "than paid for by budget cuts over my five-year budget, and this\n",
            "budget is well within the spending limits already approved by the\n",
            "Congress this year.\n",
            "             \n",
            "             It's amazing to me that many of these same senators\n",
            "who are filibustering the bill voted during the previous\n",
            "administration for billions of dollars of the same kind of\n",
            "emergency spending, and much of it was not designed to put the\n",
            "American people to work.  \n",
            "             \n",
            "             This is not about deficit spending.  We have offered\n",
            "a plan to cut the deficit.  This is about where your priorities\n",
            "are -- on people or on politics.  \n",
            "             \n",
            "             Keep in mind that our jobs bill is paid for dollar\n",
            "for dollar.  It is paid for by budget cuts.  And it's the\n",
            "soundest investment we can now make for ourselves and our\n",
            "children.  I urge all Americans to take another look at this jobs\n",
            "and investment program; to consider again the benefits for all of\n",
            "us when we've helped make more American partners working to\n",
            "ensure the future of our nation and the strength of our economy.\n",
            "             \n",
            "             You know, if every American who wanted a job had\n",
            "one, we wouldn't have a lot of the other problems we have in this\n",
            "country today.  This bill is not a miracle, it's a modest first\n",
            "step to try to set off a job creation explosion in this country\n",
            "again.  But it's a step we ought to take.  And it is fully paid\n",
            "for over the life of our budget.\n",
            "             \n",
            "             Tell your lawmakers what you think.  Tell them how\n",
            "important the bill is.  If it passes, we'll all be winners.\n",
            "             \n",
            "             Good morning, and thank you for listening.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# midamos la similaridad coseno con todos los documentos de train\n",
        "cossim = cosine_similarity(X_train[idx], X_train)[0]"
      ],
      "metadata": {
        "id": "Ssa9bqJ-hA_v"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# podemos ver los valores de similaridad ordenados de mayor a menos\n",
        "np.sort(cossim)[::-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_mDA7p3AzcQ",
        "outputId": "e67e7f33-3c05-4f4e-c37e-b7df93c5feca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.        , 0.70930477, 0.67474953, ..., 0.        , 0.        ,\n",
              "       0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y a qué documentos corresponden\n",
        "np.argsort(cossim)[::-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OIhDA1jAryX",
        "outputId": "85a4242c-6cf4-4e9f-95a3-1c66637e0d1f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4811,  6635,  4253, ...,  1534, 10055,  4750])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# los 5 documentos más similares:\n",
        "mostsim = np.argsort(cossim)[::-1][1:6]"
      ],
      "metadata": {
        "id": "hP7qLS4ZBLps"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# el documento original pertenece a la clase:\n",
        "newsgroups_test.target_names[y_train[idx]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QdJLHPJACvaj",
        "outputId": "33090ae2-1539-459b-c7ab-8141878e4c5e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'talk.politics.misc'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y los 5 más similares son de las clases:\n",
        "for i in mostsim:\n",
        "  print(newsgroups_test.target_names[y_train[i]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWy_73epCbFG",
        "outputId": "4a1e5ba6-70a3-4c82-bebe-ad2e56ed26c1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "talk.politics.misc\n",
            "talk.politics.misc\n",
            "talk.politics.misc\n",
            "talk.politics.misc\n",
            "talk.politics.misc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo de clasificación Naïve Bayes"
      ],
      "metadata": {
        "id": "zRoNnKwhBqzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# es muy fácil instanciar un modelo de clasificación Naïve Bayes y entrenarlo con sklearn\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "TPM0thDaLk0R",
        "outputId": "77f35def-ad22-443a-952a-4600ea995087"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# con nuestro vectorizador ya fiteado en train, vectorizamos los textos\n",
        "# del conjunto de test\n",
        "X_test = tfidfvect.transform(newsgroups_test.data)\n",
        "y_test = newsgroups_test.target\n",
        "y_pred =  clf.predict(X_test)"
      ],
      "metadata": {
        "id": "NrQjzM48Mu4T"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# el F1-score es una metrica adecuada para reportar desempeño de modelos de claificación\n",
        "# es robusta al desbalance de clases. El promediado 'macro' es el promedio de los\n",
        "# F1-score de cada clase. El promedio 'micro' es equivalente a la accuracy que no\n",
        "# es una buena métrica cuando los datasets son desbalanceados\n",
        "f1_score(y_test, y_pred, average='macro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkGJhetEPdA4",
        "outputId": "f419007b-9b5e-4c87-c7aa-af90403f8f94"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5854345727938506"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consigna del desafío 1"
      ],
      "metadata": {
        "id": "McArD4rSDR2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1**. Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
        "Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido\n",
        "la similaridad según el contenido del texto y la etiqueta de clasificación.\n",
        "\n",
        "**2**. Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación\n",
        "(f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros\n",
        "de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial\n",
        "y ComplementNB.\n",
        "\n",
        "**3**. Transponer la matriz documento-término. De esa manera se obtiene una matriz\n",
        "término-documento que puede ser interpretada como una colección de vectorización de palabras.\n",
        "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares.\n"
      ],
      "metadata": {
        "id": "lJgf6GQIIEH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Vectorizar documentos"
      ],
      "metadata": {
        "id": "-IDHy8sgui4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generamos los índices de random\n",
        "al_azar = np.random.randint(0, len(newsgroups_train.data), 5)\n",
        "print(f'Los índices al azar son: {al_azar}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WObQR9HufVQ",
        "outputId": "e98bc5b4-07b1-4da4-afbc-21689afbb383"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Los índices al azar son: [10650   715  5034   518  4084]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generamos una función que mida la similaridad y devuelva los 5 mayores\n",
        "def mayor_similaridad(idx):\n",
        "  cossim = cosine_similarity(X_train[al_azar[idx]], X_train)[0]\n",
        "  mostsim = np.argsort(cossim)[::-1][1:6]\n",
        "  return mostsim"
      ],
      "metadata": {
        "id": "7FtGZsFAwTMy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Probamos la función con el primer documento\n",
        "mayor_similaridad(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eqdx9w8UufMY",
        "outputId": "ab042fa7-97ec-45a6-a9ca-676060db5a40"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 498, 5045, 6261, 6440, 1492])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generamos una función que imprima el análisis por texto al azar\n",
        "def analisis_texto(idx):\n",
        "  print(f'Documento al azar número: {al_azar[idx]}')\n",
        "\n",
        "  # La etiqueta del texto es la siguiente\n",
        "  print(f'Etiqueta del documento al azar: {newsgroups_train.target_names[y_train[al_azar[idx]]]}')\n",
        "\n",
        "  # Imprimimos los primeros 200 caracteres del texto al azar\n",
        "  print(f'Contenido del documento al azar: \\n \"\"\"{newsgroups_train.data[al_azar[idx]][:200]}\"\"\"\\n')\n",
        "\n",
        "  # Imprimimos los índices de los documentos similares a este documento\n",
        "  print(f'Índices de los documentos similares: {mayor_similaridad(idx)}')\n",
        "\n",
        "  # Imprimimos la etiqueta de cada documento similar y los primeros 200 caracteres de cada texto similar\n",
        "  for i in range(len(al_azar)):\n",
        "    print(100*'-')\n",
        "    print(f'Etiqueta del documento similar {i+1}: {mayor_similaridad(idx)[i]} - {newsgroups_train.target_names[y_train[mayor_similaridad(idx)[i]]]}')\n",
        "    print(f'Contenido del documento similar {i+1}: \\n \"\"\"{newsgroups_train.data[mayor_similaridad(idx)[i]][:200]}\"\"\"\\n')\n"
      ],
      "metadata": {
        "id": "LoFcwrLZEirY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vemos el análisis de los textos\n",
        "for texto in range(len(al_azar)):\n",
        "  analisis_texto(texto)\n",
        "  print(100*'#')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2q4jl69uevw",
        "outputId": "08858320-1926-4dd1-d216-4219b09b00f5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Documento al azar número: 10650\n",
            "Etiqueta del documento al azar: comp.os.ms-windows.misc\n",
            "Contenido del documento al azar: \n",
            " \"\"\"---------- cut here ---------- part 02/03\n",
            "M_XN.GGHOL*(3IZ!02'C'\"YM=*][*&WT%S;)5:&$V8A= K/X@2$F[(J )CABC\n",
            "M=8H#9!C@^.0%CF]P[  )'._@V/$5S@ ?'#NW61T@A&-G1/H#C'!\\0)T7(*^Q\n",
            "M._[3L4,X=K08CH]P[$N>7<*Q\"SKV\"<>N_\"\"\"\n",
            "\n",
            "Índices de los documentos similares: [ 498 5045 6261 6440 1492]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 1: 498 - comp.os.ms-windows.misc\n",
            "Contenido del documento similar 1: \n",
            " \"\"\"---------- cut here ---------- part 01/01\n",
            "begin 644 1260wn31.exe\n",
            "M35KO 1D    & -$,__\\@ P $     ?#_'@     !0V]P>7)I9VAT(#$Y.#DM\n",
            "M,3DY,\"!02U=!4D4@26YC+B!!;&P@4FEG:'1S(%)E<V5R=F5D+@T*        \n",
            "M        _\"\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 2: 5045 - comp.os.ms-windows.misc\n",
            "Contenido del documento similar 2: \n",
            " \"\"\"---------- cut here ---------- part 03/03\n",
            "M2C3,JSG  A\\-\\($J7LUS?30XC;16@\"Y DK<(ME<#T(F6;;,3QX\";>?H!80H(\n",
            "MGNM U )D@/D&\"!YW0\"R\"<T!2V<XLFP6#F9?P@A4&:'&B AD.\"/,( ,V1#&W;\n",
            "M'<]6P- 3$BU['L:P$]FVI@<L8'Q\"W\\X&-\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 3: 6261 - talk.politics.guns\n",
            "Contenido del documento similar 3: \n",
            " \"\"\"begin 644 outOfControl.gif\n",
            "M1TE&.#=AN@*6`?```````/___RP`````N@*6`0`\"_@Q@J)O-[XQ4#IYX::Z3\n",
            "M9PUB4ZAY9`<NY59*:/!2L0C-J^W>2(>?[-C3Q70,6A`H9)&2OA=*ELQ)?RW+\n",
            "MP3HT5F%1RQ%INGZNVMF4&WR0?4+>\"OO!,-7(-SO,K#^I=2_<\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 4: 6440 - sci.crypt\n",
            "Contenido del documento similar 4: \n",
            " \"\"\"Here are four pseudo-random character generators, based on\n",
            "irreducible trinomials.  Each contains 16 separate trinomials,\n",
            "one of which is selected on initialization (there are 64\n",
            "distinct trinomials b\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 5: 1492 - comp.os.ms-windows.misc\n",
            "Contenido del documento similar 5: \n",
            " \"\"\"\n",
            "------------ Part 3 of 14 ------------\n",
            "M\"`@(\"`@(\"`@(\"`@(\"`@(G.3DY.3DY.3DY.3DY.3DY.3DY.3DY.2#IJ:FIJ:F\n",
            "M@^3DY.3DG.OKZZNKJZOKZYSDY.2#IJ9$B-RYB(C<$!\"(H#YM.LISRG-SV#HZ\n",
            "MRG/8<]C*<VW8.MC8V#K*RLIS.M@Z<]C*<]C\"\"\"\n",
            "\n",
            "####################################################################################################\n",
            "Documento al azar número: 715\n",
            "Etiqueta del documento al azar: soc.religion.christian\n",
            "Contenido del documento al azar: \n",
            " \"\"\"\n",
            "\n",
            "If I don't think my belief is right and everyone else's belief is wrong,\n",
            "then I don't have a belief. This is simply what belief means. Where does\n",
            "the authority for a belief come from? Nowhere, for a\"\"\"\n",
            "\n",
            "Índices de los documentos similares: [ 5452  3633  5951   210 10924]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 1: 5452 - soc.religion.christian\n",
            "Contenido del documento similar 1: \n",
            " \"\"\"\n",
            "\n",
            "Well, despite what my mother told me about accepting dares, here goes.\n",
            " \n",
            "You have to be very careful about what you mean by \"question authority\".\n",
            "Taken literally, it is nonsense. That which is autho\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 2: 3633 - alt.atheism\n",
            "Contenido del documento similar 2: \n",
            " \"\"\"<In article <31MAR199321091163@juliet.caltech.edu<, lmh@juliet.caltech.edu (Henling, Lawrence M.) writes...\n",
            "<<Atheism (Greek 'a' not + 'theos' god)  Belief that there is no god.\n",
            "<<Agnosticism (Greek '\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 3: 5951 - talk.politics.guns\n",
            "Contenido del documento similar 3: \n",
            " \"\"\"\n",
            "\n",
            "\n",
            "     I HEARTILY agree.  Now that the BATF warrant has been \n",
            "     unsealed, it is CLEAR that Clinton and Reno supported an\n",
            "     ILLEGAL raid.  Did they not KNOW this?\n",
            "\n",
            "\n",
            "\n",
            "     NO authority for a 'no-\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 4: 210 - soc.religion.christian\n",
            "Contenido del documento similar 4: \n",
            " \"\"\"To what follows, our moderator has already answered the charge of \n",
            "arrogance more ably that I could have done so, so I will confine\n",
            "myself to answering the charge of illogic.\n",
            " \n",
            "\n",
            "\n",
            "This is how everyone \"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 5: 10924 - alt.atheism\n",
            "Contenido del documento similar 5: \n",
            " \"\"\":P>My atheism is incidental, and the question of \"God\" is trivial.\n",
            ":P\n",
            ":P>But........\n",
            ":P\n",
            ":P>It matters a great deal to me when idiots try to force their belief on me,\n",
            ":P>when they try to enforce their \"\"\"\n",
            "\n",
            "####################################################################################################\n",
            "Documento al azar número: 5034\n",
            "Etiqueta del documento al azar: sci.med\n",
            "Contenido del documento al azar: \n",
            " \"\"\"\n",
            "The speculum is the little cone that fits on the end of the otoscope.\n",
            "There are also vaginal specula that females and gynecologists are\n",
            "all too familiar with.\n",
            "-- \n",
            "------------------------------------\"\"\"\n",
            "\n",
            "Índices de los documentos similares: [8550 8660 3652 2189 9124]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 1: 8550 - sci.med\n",
            "Contenido del documento similar 1: \n",
            " \"\"\"\n",
            "\n",
            "So just what was it you wanted to say?\n",
            "\n",
            "\n",
            "\n",
            "-- \n",
            "----------------------------------------------------------------------------\n",
            "Gordon Banks  N3JXP      | \"Skepticism is the chastity of the intellect, an\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 2: 8660 - sci.med\n",
            "Contenido del documento similar 2: \n",
            " \"\"\"\n",
            "\n",
            "By law, they would not be allowed to do that anyhow.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "-- \n",
            "----------------------------------------------------------------------------\n",
            "Gordon Banks  N3JXP      | \"Skepticism is the chastity of th\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 3: 3652 - sci.med\n",
            "Contenido del documento similar 3: \n",
            " \"\"\"\n",
            "\n",
            "\"Diet Evangelist\".  Good term.  Fits Atkins to a \"T\".  \n",
            "\n",
            "\n",
            "-- \n",
            "----------------------------------------------------------------------------\n",
            "Gordon Banks  N3JXP      | \"Skepticism is the chastity of t\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 4: 2189 - sci.med\n",
            "Contenido del documento similar 4: \n",
            " \"\"\"\n",
            "Senile keratoses.  Have nothing to do with the liver.\n",
            "\n",
            "\n",
            "-- \n",
            "----------------------------------------------------------------------------\n",
            "Gordon Banks  N3JXP      | \"Skepticism is the chastity of the \"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 5: 9124 - sci.med\n",
            "Contenido del documento similar 5: \n",
            " \"\"\"\n",
            "\n",
            "There is no database for infantile spasms, nor a newsgroup, that I\n",
            "know of.  The medical library will be the best source of information\n",
            "for you.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "-- \n",
            "---------------------------------------------\"\"\"\n",
            "\n",
            "####################################################################################################\n",
            "Documento al azar número: 518\n",
            "Etiqueta del documento al azar: comp.sys.mac.hardware\n",
            "Contenido del documento al azar: \n",
            " \"\"\"Is it possible, ie via creative cable splicing or whatever, to\n",
            "hook a Syquest 44MB removable drive to a Mac?\n",
            "\n",
            "Is there any difference with the guts of the drive or is it\n",
            "just cable differences?\n",
            "\n",
            "Thank\"\"\"\n",
            "\n",
            "Índices de los documentos similares: [3027 3725 2734 2726 2217]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 1: 3027 - comp.sys.ibm.pc.hardware\n",
            "Contenido del documento similar 1: \n",
            " \"\"\"I just bought a new IDE hard drive for my system to go with the one\n",
            "I already had.  My problem is this.  My system only had a IDE cable\n",
            "for one drive, so I had to buy cable with two drive connectors\n",
            "o\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 2: 3725 - comp.sys.ibm.pc.hardware\n",
            "Contenido del documento similar 2: \n",
            " \"\"\"I have a 486sx25 computer with a 105 Mg Seagate IDE drive and a controler  \n",
            "built into the motherboard. I want to add a SCSI drive (a quantum prodrive  \n",
            "425F 425 MG formatted). I have no documentation\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 3: 2734 - comp.sys.mac.hardware\n",
            "Contenido del documento similar 3: \n",
            " \"\"\"I remember reading a thread a few days ago that mentioned removing an external\n",
            "syquest drive from its case and dropping it in the internal drive of a Centris.\n",
            ". . I was going to do that with my 610, b\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 4: 2726 - comp.sys.mac.hardware\n",
            "Contenido del documento similar 4: \n",
            " \"\"\"terminated\n",
            "\n",
            "It is very possible to connect another internal hard disk in any\n",
            "macintosh if you can find the space to put it. I have a IIsi that came\n",
            "with a Quantum 80 meg drive. When I ran into space p\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 5: 2217 - comp.sys.mac.hardware\n",
            "Contenido del documento similar 5: \n",
            " \"\"\"terminated\n",
            "\n",
            "It is very possible to connect another internal hard disk in any\n",
            "macintosh if you can find the space to put it. I have a IIsi that came\n",
            "with a Quantum 80 meg drive. When I ran into space p\"\"\"\n",
            "\n",
            "####################################################################################################\n",
            "Documento al azar número: 4084\n",
            "Etiqueta del documento al azar: rec.motorcycles\n",
            "Contenido del documento al azar: \n",
            " \"\"\"\n",
            "\tIs there no JUSTICE?!\n",
            "\n",
            "\tIf I lost my leg when I was 19, and had to give up motorcycling\n",
            "(assuming David didn't know that it can be done one-legged,) I too would want\n",
            "to get swamped.... maybe even fo\"\"\"\n",
            "\n",
            "Índices de los documentos similares: [9623 4401 1292 9670 6437]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 1: 9623 - talk.politics.mideast\n",
            "Contenido del documento similar 1: \n",
            " \"\"\"Accounts of Anti-Armenian Human Right Violations in Azerbaijan #012\n",
            "                 Prelude to Current Events in Nagorno-Karabakh\n",
            "\n",
            "        +---------------------------------------------------------+\n",
            "\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 2: 4401 - misc.forsale\n",
            "Contenido del documento similar 2: \n",
            " \"\"\"Fellow netters,\n",
            "\n",
            "I just wanted to let you know that there are a few honest and good people out\n",
            "there (even outside of Iowa).  I'm sorry if anyone thinks that I am wasting\n",
            "space, but I thought you migh\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 3: 1292 - talk.politics.mideast\n",
            "Contenido del documento similar 3: \n",
            " \"\"\"Accounts of Anti-Armenian Human Right Violations in Azerbaijan #008 Part B\n",
            "                 Prelude to Current Events in Nagorno-Karabakh\n",
            "\n",
            "\t\t\t\t(Part B of #008)\n",
            "\n",
            "      +--------------------------------\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 4: 9670 - soc.religion.christian\n",
            "Contenido del documento similar 4: \n",
            " \"\"\"\n",
            "[..]\n",
            "\n",
            "\n",
            "Hello.  Firstly, what do you exactly mean by \"fundamentalist\"?  I will\n",
            "for the time being assume that what you mean is that your friend believes\n",
            "that the bible is God's word to mankind?  I sus\"\"\"\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Etiqueta del documento similar 5: 6437 - talk.politics.mideast\n",
            "Contenido del documento similar 5: \n",
            " \"\"\"Accounts of Anti-Armenian Human Rights Violations in Azerbaijan #007\n",
            "                 Prelude to Current Events in Nagorno-Karabakh\n",
            "\n",
            "\n",
            " +----------------------------------------------------------------\"\"\"\n",
            "\n",
            "####################################################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- En la mayoría de los casos, se entiende por contexto que la similaridad de coseno es una buena herramienta para entender qué tan cerca está el contexto de lo que se habla.\n",
        "- Existe un caso en que tema del texto original al azar es de religión, pero el tema es política."
      ],
      "metadata": {
        "id": "Bo6gXLg1Qoc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Modelos de Clasificación Naïve-Bayes"
      ],
      "metadata": {
        "id": "fp1U96RCRJzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generamos una función que tome un vectorizador y un clasificador y retorne un F1 Score\n",
        "def f1_vectoriz(vectorizador=TfidfVectorizer(), clasificador=MultinomialNB()):\n",
        "  # Train\n",
        "  X_train = vectorizador.fit_transform(newsgroups_train.data)\n",
        "  y_train = newsgroups_train.target\n",
        "\n",
        "  # Test\n",
        "  X_test = vectorizador.transform(newsgroups_test.data)\n",
        "  y_test = newsgroups_test.target\n",
        "\n",
        "  # Modelo de clasificación\n",
        "  clasificador = MultinomialNB()\n",
        "  clasificador.fit(X_train, y_train)\n",
        "\n",
        "  # Prediction\n",
        "  y_pred = clasificador.predict(X_test)\n",
        "\n",
        "  # F1_Score\n",
        "  return f1_score(y_test, y_pred, average='macro')"
      ],
      "metadata": {
        "id": "3U8Oc_LMueaP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Probamos la función con el modelo default\n",
        "f1_vectoriz()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIBbGehRueUU",
        "outputId": "7fb9bef5-3459-4b9f-c2e1-7bbf09cd5200"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5854345727938506"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1) Vectorizador: parámetros"
      ],
      "metadata": {
        "id": "SFuWV4mIpNsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Planteamos un vectorizer con 50000 features, menor que el vocabulario inicial 101631\n",
        "vect_1 = TfidfVectorizer(max_features=50000)\n",
        "\n",
        "# Generamos el F1 Score\n",
        "f1_vectoriz(vectorizador=vect_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhm_VtXQoKeJ",
        "outputId": "bcc060f8-81d2-40c7-a7ea-fd4258797fcd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.593410793017284"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Planteamos un vectorizer con 20000 features, menor que el vocabulario inicial 101631\n",
        "vect_2 = TfidfVectorizer(max_features=20000)\n",
        "\n",
        "# Generamos el F1 Score\n",
        "f1_vectoriz(vectorizador=vect_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzyaJ8xToKZK",
        "outputId": "b36cacf4-0d01-4dfa-da83-b44a828a62ea"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6078865750826256"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Disminuyendo la cantidad de features, aumentamos el valor de F1 Score. Reducir la dimensionalidad puede mejorar la eficiencia y evitar el sobreajuste."
      ],
      "metadata": {
        "id": "0HB5uGAoo-Z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modifcar a False el valor de use_idf\n",
        "vect_3 = TfidfVectorizer(use_idf=False)\n",
        "\n",
        "# Generamos el F1 Score\n",
        "f1_vectoriz(vectorizador=vect_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEdnn2yhoKQx",
        "outputId": "ce4c949f-cff7-4095-82d9-f729a3871a08"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4715859558342732"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Si quitamos el use_idf vemos que el F1 score baja. La ponderación IDF puede mejorar la precisión al dar más importancia a las palabras que son menos comunes en el corpus."
      ],
      "metadata": {
        "id": "fAXZZlC7qUmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modificamos el analyzer\n",
        "vect_4 = TfidfVectorizer(analyzer='char')\n",
        "\n",
        "# Generamos el F1 Score\n",
        "f1_vectoriz(vectorizador=vect_4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hd8KttunpyBy",
        "outputId": "4f85d802-ca02-4ac6-bf76-3ba0f1fc9558"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11828114898439623"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- En caso de dividirlo en caracteres me disminuye el F1 score ya que no tiene mucho sentido en este caso."
      ],
      "metadata": {
        "id": "DkwAT3_LrOd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usamos la siguiente lista de stop words --> fuente: https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py\n",
        "# Stop words\n",
        "STOP_WORDS = set(\n",
        "    \"\"\"\n",
        "a about above across after afterwards again against all almost alone along\n",
        "already also although always am among amongst amount an and another any anyhow\n",
        "anyone anything anyway anywhere are around as at\n",
        "\n",
        "back be became because become becomes becoming been before beforehand behind\n",
        "being below beside besides between beyond both bottom but by\n",
        "\n",
        "call can cannot ca could\n",
        "\n",
        "did do does doing done down due during\n",
        "\n",
        "each eight either eleven else elsewhere empty enough even ever every\n",
        "everyone everything everywhere except\n",
        "\n",
        "few fifteen fifty first five for former formerly forty four from front full\n",
        "further\n",
        "\n",
        "get give go\n",
        "\n",
        "had has have he hence her here hereafter hereby herein hereupon hers herself\n",
        "him himself his how however hundred\n",
        "\n",
        "i if in indeed into is it its itself\n",
        "\n",
        "keep\n",
        "\n",
        "last latter latterly least less ll\n",
        "\n",
        "just\n",
        "\n",
        "made make many may me meanwhile might mine more moreover most mostly move much\n",
        "must my myself\n",
        "\n",
        "name namely neither never nevertheless next nine no nobody none noone nor not\n",
        "nothing now nowhere\n",
        "\n",
        "of off often on once one only onto or other others otherwise our ours ourselves\n",
        "out over own\n",
        "\n",
        "part per perhaps please put\n",
        "\n",
        "quite\n",
        "\n",
        "rather re really regarding\n",
        "\n",
        "same say see seem seemed seeming seems serious several she should show side\n",
        "since six sixty so some somehow someone something sometime sometimes somewhere\n",
        "still such\n",
        "\n",
        "take ten than that the their them themselves then thence there thereafter\n",
        "thereby therefore therein thereupon these they third this those though three\n",
        "through throughout thru thus to together too top toward towards twelve twenty\n",
        "two\n",
        "\n",
        "under until up unless upon us used using\n",
        "\n",
        "various ve very very via was we well were what whatever when whence whenever where\n",
        "whereafter whereas whereby wherein whereupon wherever whether which while\n",
        "whither who whoever whole whom whose why will with within without would\n",
        "\n",
        "yet you your yours yourself yourselves\n",
        "\"\"\".split()\n",
        ")\n",
        "\n",
        "contractions = [\"n't\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\"]\n",
        "STOP_WORDS.update(contractions)\n",
        "\n",
        "for apostrophe in [\"‘\", \"’\"]:\n",
        "    for stopword in contractions:\n",
        "        STOP_WORDS.add(stopword.replace(\"'\", apostrophe))\n",
        "STOP_WORDS = list(STOP_WORDS)"
      ],
      "metadata": {
        "id": "KzJhssxFpx61"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modificamos el vectorizador con esta lista\n",
        "vect_5 = TfidfVectorizer(stop_words=STOP_WORDS)\n",
        "\n",
        "# Generamos el F1 Score\n",
        "f1_vectoriz(vectorizador=vect_5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXpJ-7i7pxzr",
        "outputId": "7a9eee17-696a-48a1-bada-eac81e4c6e16"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6489210209931484"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Mejora notablemente en este caso ya que eliminar palabras vacías tiene impacto positivo en la eficiencia y la precisión del modelo al enfocarse en términos más relevantes."
      ],
      "metadata": {
        "id": "glEX3-gns0zr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2) Modelo MultinomialNB: parámetros"
      ],
      "metadata": {
        "id": "DuVOWH4Ks6O9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilizamos la misma función f1_vectoriz pero\n",
        "# modificamos parámetros del modelo MultinomialNB\n",
        "# Usamos como parámetros del vectorizador la constante STOP_WORDS: vect_5\n",
        "\n",
        "# Modificamos alpha\n",
        "alpha = 0.9\n",
        "clasif_2 = MultinomialNB(alpha=alpha)\n",
        "\n",
        "# Calculamos el f1_score\n",
        "f1_vectoriz(vectorizador=vect_5, clasificador=clasif_2)\n"
      ],
      "metadata": {
        "id": "p5ekvmYVs0kz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3693440-500d-4675-a889-07f2c8659313"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6489210209931484"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usamos un nuevo alpha más chico\n",
        "# Modificamos alpha\n",
        "alpha = 0.7\n",
        "clasif_3 = MultinomialNB(alpha=alpha)\n",
        "\n",
        "# Calculamos el f1_score\n",
        "f1_vectoriz(vectorizador=vect_5, clasificador=clasif_3)"
      ],
      "metadata": {
        "id": "yynRJqTws0Wa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c2b94d-192b-4ce2-a886-d291ca4c4fdb"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6489210209931484"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Al bajar este parámetro, no tenemos una mejora en el score final. Esto puede ser así, al no tener un sobreajuste."
      ],
      "metadata": {
        "id": "mhVaYv2QvonD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modificamos fit prior\n",
        "fit_prior = False\n",
        "clasif_4 = MultinomialNB(fit_prior=fit_prior)\n",
        "\n",
        "# Calculamos el f1_score\n",
        "f1_vectoriz(vectorizador=vect_5, clasificador=clasif_4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__5Mt9aUvKK6",
        "outputId": "a1beb2f1-ab17-4214-9f95-9adffadd85de"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6489210209931484"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- No tenemos un cambio visible."
      ],
      "metadata": {
        "id": "8feF8oirxsH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3) Modelo ComplementNB"
      ],
      "metadata": {
        "id": "f2IBRp2Vx1Mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usamos el mismo vectorizador y modificamos el clasificador ComplementNB\n",
        "# Usamos primero el vectorizador original\n",
        "clasif_5 = ComplementNB()\n",
        "\n",
        "# Calculamos el f1_score\n",
        "f1_vectoriz(clasificador=clasif_5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj4VWKwxx0QA",
        "outputId": "71376068-88a5-4815-c39c-5ce3b2d48a95"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5854345727938506"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usamos el vectorizador 5\n",
        "clasif_6 = ComplementNB()\n",
        "\n",
        "# Calculamos el f1_score\n",
        "f1_vectoriz(vectorizador=vect_5, clasificador=clasif_6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jb5vn9Lwx0LV",
        "outputId": "f128feba-320f-45c6-c8a9-829518e6c053"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6489210209931484"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Si bien, a priori, los valores de score de este modelo\n",
        "no son mejores a los del MultinomialNB (deberían por ser ComplementNB un corrector de las suposiciones del MultinomialNB), puede deberser al haber unsado un vectorizador de muy buena performance."
      ],
      "metadata": {
        "id": "WBBWtDdEzOd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Matriz Término-Documento"
      ],
      "metadata": {
        "id": "h6c35Tt6z5vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Planteamos la transposición de las palabras vectorizadas\n",
        "tfidfvect = TfidfVectorizer()\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
        "X_train = X_train.T"
      ],
      "metadata": {
        "id": "uZdNYxJ2x0Dx"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vemos entonces el shape de la T\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXYMpvMrxzow",
        "outputId": "37cc131f-c214-4ceb-f91f-fb6a97a9ee83"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(101631, 11314)"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generamos los índices de random\n",
        "al_azar = np.random.randint(0, X_train.shape[0], 5)\n",
        "print(f'Los índices al azar son: {al_azar}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XJklrq6JxAs",
        "outputId": "707d8061-511c-4db4-e61d-85eb5eb5aa72"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Los índices al azar son: [30063 78091 70840 21945 91625]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Es muy útil tener el diccionario opuesto que va de índices a términos\n",
        "idx2word = {v: k for k,v in tfidfvect.vocabulary_.items()}"
      ],
      "metadata": {
        "id": "sXvoaVOSOnVu"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Genero una función que busque la palabra dentro del vocabulario\n",
        "def buscar_palabra(idx):\n",
        "  vocab = idx2word\n",
        "  for indice, palabra in vocab.items():\n",
        "    if indice == idx:\n",
        "      return palabra\n",
        "  return None"
      ],
      "metadata": {
        "id": "uEIiGERmK1y4"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generamos una función que imprima el análisis por palabra al azar\n",
        "def analisis_palabra(idx):\n",
        "  print(f'Palabra al azar número: {al_azar[idx]}')\n",
        "\n",
        "  # Imprimimos la palabra buscada\n",
        "  print(f'La palabra es: {buscar_palabra(al_azar[idx])}')\n",
        "\n",
        "  # Imprimimos los índices de las palabras similares a esta\n",
        "  print(f'Índices de las Palabras similares: {mayor_similaridad(idx)}')\n",
        "\n",
        "  # Imprimimos las palabras similares\n",
        "  for i in range(len(al_azar)):\n",
        "    print(f'Palabras similares: {buscar_palabra(mayor_similaridad(idx)[i])}')\n"
      ],
      "metadata": {
        "id": "n7Y7dx0yK1_G"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vemos el análisis de las palabras y sus similares\n",
        "for palabra in range(len(al_azar)):\n",
        "  analisis_palabra(palabra)\n",
        "  print(100*'#')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzjgY2-jK16q",
        "outputId": "9cead192-2bc7-4ad1-9eb9-f8a1740ba0b4"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabra al azar número: 30063\n",
            "La palabra es: cotton\n",
            "Índices de las Palabras similares: [38309 60655 97419 52486 71765]\n",
            "Palabras similares: evicted\n",
            "Palabras similares: mechanization\n",
            "Palabras similares: wrath_\n",
            "Palabras similares: joad\n",
            "Palabras similares: plantations\n",
            "####################################################################################################\n",
            "Palabra al azar número: 78091\n",
            "La palabra es: reversal\n",
            "Índices de las Palabras similares: [15332 14889 49234 98499 78096]\n",
            "Palabras similares: _new_\n",
            "Palabras similares: _bloody_hell_no_\n",
            "Palabras similares: imnsho\n",
            "Palabras similares: xians\n",
            "Palabras similares: reversible\n",
            "####################################################################################################\n",
            "Palabra al azar número: 70840\n",
            "La palabra es: personailty\n",
            "Índices de las Palabras similares: [63169 26767 70840 43876 82002]\n",
            "Palabras similares: morte\n",
            "Palabras similares: charater\n",
            "Palabras similares: personailty\n",
            "Palabras similares: goffer\n",
            "Palabras similares: shakespear\n",
            "####################################################################################################\n",
            "Palabra al azar número: 21945\n",
            "La palabra es: bctel\n",
            "Índices de las Palabras similares: [ 4497 33464 77924 57010 66857]\n",
            "Palabras similares: 211\n",
            "Palabras similares: dial\n",
            "Palabras similares: result\n",
            "Palabras similares: live\n",
            "Palabras similares: number\n",
            "####################################################################################################\n",
            "Palabra al azar número: 91625\n",
            "La palabra es: umt\n",
            "Índices de las Palabras similares: [74819  8758 74844 74840  8757]\n",
            "Palabras similares: qiw_\n",
            "Palabras similares: 52eutwr\n",
            "Palabras similares: qkafs\n",
            "Palabras similares: qk5\n",
            "Palabras similares: 52eg\n",
            "####################################################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Planteamos lo mismo con un número menor de features y analizamos nuevamente\n",
        "tfidfvect_10000 = TfidfVectorizer(max_features=10000)\n",
        "X_train = tfidfvect_10000.fit_transform(newsgroups_train.data)\n",
        "X_train = X_train.T\n",
        "\n",
        "# Vemos entonces el shape de la T\n",
        "print(f'El shape es: {X_train.shape}\\n')\n",
        "\n",
        "# Generamos los índices de random\n",
        "al_azar = np.random.randint(0, X_train.shape[0], 5)\n",
        "print(f'Los índices al azar son: {al_azar}')\n",
        "\n",
        "# Es muy útil tener el diccionario opuesto que va de índices a términos\n",
        "idx2word_10000 = {v: k for k,v in tfidfvect_10000.vocabulary_.items()}\n",
        "\n",
        "# Genero una función que busque la palabra dentro del vocabulario\n",
        "def buscar_palabra(idx):\n",
        "  vocab = idx2word_10000\n",
        "  for indice, palabra in vocab.items():\n",
        "    if indice == idx:\n",
        "      return palabra\n",
        "  return None\n",
        "\n",
        "# Vemos el análisis de las palabras y sus similares\n",
        "for palabra in range(len(al_azar)):\n",
        "  analisis_palabra(palabra)\n",
        "  print(100*'#')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw6HYuuJVk5E",
        "outputId": "3968ae43-f149-4929-95a1-e2c6cd55156a"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El shape es: (10000, 11314)\n",
            "\n",
            "Los índices al azar son: [5445 4525 5702 1548 3308]\n",
            "Palabra al azar número: 5445\n",
            "La palabra es: linked\n",
            "Índices de las Palabras similares: [9558 2016  997 4125 1451]\n",
            "Palabras similares: vpic46\n",
            "Palabras similares: cfg\n",
            "Palabras similares: allocation\n",
            "Palabras similares: gfx\n",
            "Palabras similares: barrier\n",
            "####################################################################################################\n",
            "Palabra al azar número: 4525\n",
            "La palabra es: hope\n",
            "Índices de las Palabras similares: [4425 9026 8917 8919 9949]\n",
            "Palabras similares: helps\n",
            "Palabras similares: to\n",
            "Palabras similares: that\n",
            "Palabras similares: the\n",
            "Palabras similares: you\n",
            "####################################################################################################\n",
            "Palabra al azar número: 5702\n",
            "La palabra es: master\n",
            "Índices de las Palabras similares: [8264  807 2729 1720 5103]\n",
            "Palabras similares: slave\n",
            "Palabras similares: accord\n",
            "Palabras similares: cylinder\n",
            "Palabras similares: brake\n",
            "Palabras similares: jumper\n",
            "####################################################################################################\n",
            "Palabra al azar número: 1548\n",
            "La palabra es: beranek\n",
            "Índices de las Palabras similares: [1703 9274 7422 4375 4049]\n",
            "Palabras similares: bowen\n",
            "Palabras similares: unassisted\n",
            "Palabras similares: recchi\n",
            "Palabras similares: hawgood\n",
            "Palabras similares: galley\n",
            "####################################################################################################\n",
            "Palabra al azar número: 3308\n",
            "La palabra es: education\n",
            "Índices de las Palabras similares: [6989 5392 4845 8000 3744]\n",
            "Palabras similares: pregnancy\n",
            "Palabras similares: liberal\n",
            "Palabras similares: institution\n",
            "Palabras similares: secretary\n",
            "Palabras similares: federalist\n",
            "####################################################################################################\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Si vamos con el número de features original (vocabulario 101631) muchas de las palabras dentro del diccionario son palabras que no forman parte del vocabulario Inglés persé, sino palabras que quedaron dentro del vocabulario del texto como \"malos tokens\".\n",
        "- Incluso, bajando el número de features, tenemos estas palabras pero en menor proporción.\n",
        "- Al ir con 10000 palabras, si bien el resultado de las similares son palabras del inglés, no existe mucha similaridad contextual entre ellas."
      ],
      "metadata": {
        "id": "854LbdaeTHC9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f9EfXeq8Tkxm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}