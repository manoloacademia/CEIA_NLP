{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 3782728,
          "sourceType": "datasetVersion",
          "datasetId": 1692
        }
      ],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import io\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout, Conv1D, Bidirectional, MaxPooling1D, GRU, Reshape\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:08.285291Z",
          "iopub.execute_input": "2024-04-21T14:13:08.285632Z",
          "iopub.status.idle": "2024-04-21T14:13:08.292401Z",
          "shell.execute_reply.started": "2024-04-21T14:13:08.285596Z",
          "shell.execute_reply": "2024-04-21T14:13:08.291393Z"
        },
        "trusted": true,
        "id": "dvvYqAd6QxUA"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6w9VAXINQ8Q7",
        "outputId": "47292e8f-4741-4e4f-be2b-05e9240e1754"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Desafio 3 - Pablo Segovia"
      ],
      "metadata": {
        "id": "TPx_R02EQxUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consigna"
      ],
      "metadata": {
        "id": "1eItcTAMQxUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Entrenar un modelo de lenguaje basado en arquitectura recurrente, utilizando un corpus mayor al utilizado en clase.\n",
        "- Explorar diferentes variantes de arquitecturas (GRU, LSTM, Bidireccionales, Cantidad de capas y neuronas, tamaño de contexto máximo)\n",
        "- El objetivo principal sería lograr que durante el entrenamiento baje la perplejidad.\n",
        "- Con el modelo que les haya resultado mejor, realizar algunas pruebas de generación de secuencias utilizando greedy search, beam search determinista y beam search estocástico (aquí pueden incluso variar la temperatura)."
      ],
      "metadata": {
        "id": "nZVDQ_REQxUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Corpus elegido\n",
        "- Se elige el corpus one_millon_headlines de Kaggle.\n",
        "- https://www.kaggle.com/datasets/therohk/million-headlines/data"
      ],
      "metadata": {
        "id": "MvgS-5fQQxUF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploración del corpus"
      ],
      "metadata": {
        "id": "elod6n42QxUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Leemos el dataset\n",
        "datafile = '/content/drive/MyDrive/Colab Notebooks/CEIA/04 - NLP/abcnews-date-text.csv'\n",
        "raw_df = pd.read_csv(datafile, parse_dates=[0], infer_datetime_format=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:08.293674Z",
          "iopub.execute_input": "2024-04-21T14:13:08.293946Z",
          "iopub.status.idle": "2024-04-21T14:13:09.725040Z",
          "shell.execute_reply.started": "2024-04-21T14:13:08.293924Z",
          "shell.execute_reply": "2024-04-21T14:13:09.724243Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKMNGqPXQxUG",
        "outputId": "b5c78921-85b9-4987-eae4-54da8dada18a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-4f603a5abe05>:3: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  raw_df = pd.read_csv(datafile, parse_dates=[0], infer_datetime_format=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vemos como está definido el .csv\n",
        "raw_df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:09.727476Z",
          "iopub.execute_input": "2024-04-21T14:13:09.728063Z",
          "iopub.status.idle": "2024-04-21T14:13:09.737777Z",
          "shell.execute_reply.started": "2024-04-21T14:13:09.728024Z",
          "shell.execute_reply": "2024-04-21T14:13:09.736620Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Ic2rxWM0QxUH",
        "outputId": "cbe3a84c-32ec-4f40-a739-56b804fd9cf5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  publish_date                                      headline_text\n",
              "0   2003-02-19  aba decides against community broadcasting lic...\n",
              "1   2003-02-19     act fire witnesses must be aware of defamation\n",
              "2   2003-02-19     a g calls for infrastructure protection summit\n",
              "3   2003-02-19           air nz staff in aust strike for pay rise\n",
              "4   2003-02-19      air nz strike to affect australian travellers"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a59688ba-b96a-448d-ac29-85ccac82f0f4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>publish_date</th>\n",
              "      <th>headline_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2003-02-19</td>\n",
              "      <td>aba decides against community broadcasting lic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2003-02-19</td>\n",
              "      <td>act fire witnesses must be aware of defamation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2003-02-19</td>\n",
              "      <td>a g calls for infrastructure protection summit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2003-02-19</td>\n",
              "      <td>air nz staff in aust strike for pay rise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2003-02-19</td>\n",
              "      <td>air nz strike to affect australian travellers</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a59688ba-b96a-448d-ac29-85ccac82f0f4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a59688ba-b96a-448d-ac29-85ccac82f0f4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a59688ba-b96a-448d-ac29-85ccac82f0f4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3bf90db7-63e6-42f7-b813-7306609bafba\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3bf90db7-63e6-42f7-b813-7306609bafba')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3bf90db7-63e6-42f7-b813-7306609bafba button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "raw_df"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dejamos la columna que nos importa y calculamos la dimensión\n",
        "df = raw_df['headline_text']\n",
        "print(f\"El numero de filas (headlines) que tiene el corpus es: {len(df)}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:09.739111Z",
          "iopub.execute_input": "2024-04-21T14:13:09.739382Z",
          "iopub.status.idle": "2024-04-21T14:13:09.747478Z",
          "shell.execute_reply.started": "2024-04-21T14:13:09.739358Z",
          "shell.execute_reply": "2024-04-21T14:13:09.746576Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMIyAXH7QxUI",
        "outputId": "a12e9219-af8c-4c5c-e4c3-a4fdb9e8ebd1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El numero de filas (headlines) que tiene el corpus es: 1244184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Por una cuestión de tiempo de procesamiento, tomamos un 1% del corpus\n",
        "tenperc_text = len(df)/100\n",
        "df = df.iloc[0:int(tenperc_text)]\n",
        "print(f'La nueva longitud del texto es: {len(df)} filas')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:09.748636Z",
          "iopub.execute_input": "2024-04-21T14:13:09.748957Z",
          "iopub.status.idle": "2024-04-21T14:13:09.757809Z",
          "shell.execute_reply.started": "2024-04-21T14:13:09.748931Z",
          "shell.execute_reply": "2024-04-21T14:13:09.756641Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMOLOOg6QxUI",
        "outputId": "a7f24592-6eb5-4926-b4c8-010c4d370847"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La nueva longitud del texto es: 12441 filas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tamaño del contexto"
      ],
      "metadata": {
        "id": "trOd-xNYQxUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cada verso lo guardamos en una lista\n",
        "text = list(df)\n",
        "print(f'Vemos los primero headlines de la lista: {text[:10]}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:09.759078Z",
          "iopub.execute_input": "2024-04-21T14:13:09.759466Z",
          "iopub.status.idle": "2024-04-21T14:13:09.783457Z",
          "shell.execute_reply.started": "2024-04-21T14:13:09.759431Z",
          "shell.execute_reply": "2024-04-21T14:13:09.782361Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0lhomt1QxUJ",
        "outputId": "a7dd9b0a-a2e5-4bd5-d7bb-cd0b6a1cc8cc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vemos los primero headlines de la lista: ['aba decides against community broadcasting licence', 'act fire witnesses must be aware of defamation', 'a g calls for infrastructure protection summit', 'air nz staff in aust strike for pay rise', 'air nz strike to affect australian travellers', 'ambitious olsson wins triple jump', 'antic delighted with record breaking barca', 'aussie qualifier stosur wastes four memphis match', 'aust addresses un security council over iraq', 'australia is locked into war timetable opp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usaremos las utilidades de procesamiento de textos y secuencias de Keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # equivalente a ltokenizer de nltk\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence # equivalente a word_tokenize de nltk\n",
        "from tensorflow.keras.utils import pad_sequences # se utilizará para padding"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:09.787055Z",
          "iopub.execute_input": "2024-04-21T14:13:09.787332Z",
          "iopub.status.idle": "2024-04-21T14:13:09.792639Z",
          "shell.execute_reply.started": "2024-04-21T14:13:09.787308Z",
          "shell.execute_reply": "2024-04-21T14:13:09.791660Z"
        },
        "trusted": true,
        "id": "2WCjW8R1QxUK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Segmentamos el texto con la utilidad de Keras\n",
        "segmented_sentences = [text_to_word_sequence(sentence) for sentence in text]\n",
        "\n",
        "# Calculamos la longitud de cada secuencia\n",
        "length_sentences = [len(sentence) for sentence in segmented_sentences]\n",
        "\n",
        "# Podemos ver su distribución\n",
        "plt.hist(length_sentences,bins=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:09.793763Z",
          "iopub.execute_input": "2024-04-21T14:13:09.794350Z",
          "iopub.status.idle": "2024-04-21T14:13:11.961961Z",
          "shell.execute_reply.started": "2024-04-21T14:13:09.794300Z",
          "shell.execute_reply": "2024-04-21T14:13:11.961051Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "Sh7F8eBFQxUK",
        "outputId": "b8ff0949-4829-474f-a810-d87a0c5f87f3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Elegimos la mitad\n",
        "max_context_size = 6"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:11.963067Z",
          "iopub.execute_input": "2024-04-21T14:13:11.963320Z",
          "iopub.status.idle": "2024-04-21T14:13:11.967479Z",
          "shell.execute_reply.started": "2024-04-21T14:13:11.963298Z",
          "shell.execute_reply": "2024-04-21T14:13:11.966466Z"
        },
        "trusted": true,
        "id": "HJIy4q6-QxUK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizador"
      ],
      "metadata": {
        "id": "iBvIxUQSQxUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el tokenizador\n",
        "tok = Tokenizer()\n",
        "\n",
        "# El tokenizer \"aprende\" las palabras que se usaran\n",
        "# Se construye (fit) una vez por proyecto, se aplica N veces (tal cual un encoder)\n",
        "# El token 0 es reservado y no es asignado. Se utiliza para designar a palabras\n",
        "# fuera del vocabulario aprendido\n",
        "tok.fit_on_texts(segmented_sentences)\n",
        "\n",
        "# Convertimos las palabras a números\n",
        "# entran palabras -> salen números\n",
        "tokenized_sentences = tok.texts_to_sequences(segmented_sentences)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:11.968772Z",
          "iopub.execute_input": "2024-04-21T14:13:11.969146Z",
          "iopub.status.idle": "2024-04-21T14:13:13.663097Z",
          "shell.execute_reply.started": "2024-04-21T14:13:11.969113Z",
          "shell.execute_reply": "2024-04-21T14:13:13.662101Z"
        },
        "trusted": true,
        "id": "m71U3NdsQxUL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vemos las primeras 10 frases tokenizadas\n",
        "tokenized_sentences[:10]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:13.664444Z",
          "iopub.execute_input": "2024-04-21T14:13:13.665263Z",
          "iopub.status.idle": "2024-04-21T14:13:13.672630Z",
          "shell.execute_reply.started": "2024-04-21T14:13:13.665226Z",
          "shell.execute_reply": "2024-04-21T14:13:13.671752Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq06B30IQxUL",
        "outputId": "a5576ce7-f114-46ac-d3cd-33ee98659ae5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2445, 3473, 39, 166, 3474, 1885],\n",
              " [126, 30, 1553, 298, 20, 3475, 4, 1418],\n",
              " [35, 1554, 92, 3, 2119, 945, 378],\n",
              " [139, 210, 362, 2, 64, 185, 3, 167, 214],\n",
              " [139, 210, 185, 1, 1022, 135, 2446],\n",
              " [6156, 6157, 111, 1555, 1694],\n",
              " [6158, 3476, 12, 177, 2120, 2888],\n",
              " [379, 2447, 6159, 6160, 158, 2889, 222],\n",
              " [64, 2448, 51, 93, 24, 7, 8],\n",
              " [72, 136, 4372, 33, 9, 6161, 481]]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Organización y estructurda del dataset"
      ],
      "metadata": {
        "id": "gn7qhuvnQxUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usamos el train_test_split de skleant\n",
        "tokenized_sentences_train, tokenized_sentences_val, _, _ = train_test_split(tokenized_sentences, tokenized_sentences, test_size=0.2, random_state=17)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:13.673516Z",
          "iopub.execute_input": "2024-04-21T14:13:13.673834Z",
          "iopub.status.idle": "2024-04-21T14:13:13.744807Z",
          "shell.execute_reply.started": "2024-04-21T14:13:13.673810Z",
          "shell.execute_reply": "2024-04-21T14:13:13.744079Z"
        },
        "trusted": true,
        "id": "6XHh654eQxUL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizamos un separar las oraciones que tienen el tamaño más grande que el máximo tamaño de contexto\n",
        "tok_sent = []\n",
        "\n",
        "for sent in tokenized_sentences_train:\n",
        "\n",
        "  # si la secuencia tiene más términos que el tamaño de contexto máximo,\n",
        "  # armo varias sub-secuencias de tamaño máximo\n",
        "  if len(sent) > (max_context_size+1):\n",
        "    extra = len(sent)-(max_context_size+1) + 1\n",
        "    for i in range(extra):\n",
        "      tok_sent.append(sent[i:i+max_context_size+1])\n",
        "  else: # si la secuencia tiene menos términos el tamaño de contexto máximo, dejo la secuencia como está\n",
        "    tok_sent.append(sent)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:13.745827Z",
          "iopub.execute_input": "2024-04-21T14:13:13.746088Z",
          "iopub.status.idle": "2024-04-21T14:13:13.787586Z",
          "shell.execute_reply.started": "2024-04-21T14:13:13.746060Z",
          "shell.execute_reply": "2024-04-21T14:13:13.786830Z"
        },
        "trusted": true,
        "id": "BYj8kxKoQxUM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizamos un data augmentation de estas oraciones\n",
        "tok_sent_augm = []\n",
        "\n",
        "for sent in tok_sent:\n",
        "\n",
        "  # generamos todas las sub-secuencias\n",
        "  subseq = [sent[:i+2] for i in range(len(sent)-1)]\n",
        "  # en esta línea paddeamos al tamaño de contexto máximo\n",
        "  tok_sent_augm.append(pad_sequences(subseq, maxlen=max_context_size+1, padding='pre'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:13.788607Z",
          "iopub.execute_input": "2024-04-21T14:13:13.788944Z",
          "iopub.status.idle": "2024-04-21T14:13:17.172240Z",
          "shell.execute_reply.started": "2024-04-21T14:13:13.788912Z",
          "shell.execute_reply": "2024-04-21T14:13:17.171462Z"
        },
        "trusted": true,
        "id": "nc_Tgb6hQxUM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizamos una concatenación de las secuencias y vemo su shape\n",
        "train_seqs = np.concatenate(tok_sent_augm, axis=0)\n",
        "print(f\"Shape del dataset segmentado y aumentado: {train_seqs.shape}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:17.173269Z",
          "iopub.execute_input": "2024-04-21T14:13:17.173554Z",
          "iopub.status.idle": "2024-04-21T14:13:17.242724Z",
          "shell.execute_reply.started": "2024-04-21T14:13:17.173530Z",
          "shell.execute_reply": "2024-04-21T14:13:17.241811Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpbtpUlzQxUM",
        "outputId": "59293810-195e-4a79-d685-6ca7caebac89"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape del dataset segmentado y aumentado: (68475, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generamos el X y el y\n",
        "X = train_seqs[:,:-1]\n",
        "y = train_seqs[:,-1]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:17.243900Z",
          "iopub.execute_input": "2024-04-21T14:13:17.244267Z",
          "iopub.status.idle": "2024-04-21T14:13:17.249438Z",
          "shell.execute_reply.started": "2024-04-21T14:13:17.244231Z",
          "shell.execute_reply": "2024-04-21T14:13:17.248442Z"
        },
        "trusted": true,
        "id": "qBKiO4K7QxUM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vemos cuantas palabras tiene el vocabulario\n",
        "# Cantidad de palabras en el vocabulario\n",
        "vocab_size = len(tok.word_counts)\n",
        "vocab_size"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:17.250855Z",
          "iopub.execute_input": "2024-04-21T14:13:17.251118Z",
          "iopub.status.idle": "2024-04-21T14:13:17.260223Z",
          "shell.execute_reply.started": "2024-04-21T14:13:17.251097Z",
          "shell.execute_reply": "2024-04-21T14:13:17.259262Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI2J1u_jQxUN",
        "outputId": "f5780fe2-d956-4453-d545-394a56f3eb6c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11248"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callback de Perplexity"
      ],
      "metadata": {
        "id": "BR_xf02qQxUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el callbak para el cálculo de la Perplexity\n",
        "class PplCallback(keras.callbacks.Callback):\n",
        "\n",
        "    '''\n",
        "    Este callback es una solución ad-hoc para calcular al final de cada epoch de\n",
        "    entrenamiento la métrica de Perplejidad sobre un conjunto de datos de validación.\n",
        "    La perplejidad es una métrica cuantitativa para evaluar la calidad de la generación de secuencias.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, val_data):\n",
        "      # El callback lo inicializamos con secuencias de validación sobre las cuales\n",
        "      # mediremos la perplejidad\n",
        "      self.val_data = val_data\n",
        "\n",
        "      self.target = []\n",
        "      self.padded = []\n",
        "\n",
        "      count = 0\n",
        "      self.info = []\n",
        "\n",
        "      # nos movemos en todas las secuencias de los datos de validación\n",
        "      for seq in self.val_data:\n",
        "\n",
        "        len_seq = len(seq)\n",
        "        # armamos todas las subsecuencias\n",
        "        subseq = [seq[:i] for i in range(len_seq)]\n",
        "        self.target.extend([seq[i] for i in range(len_seq)])\n",
        "        self.padded.append(pad_sequences(subseq, maxlen=max_context_size, padding='pre'))\n",
        "\n",
        "        self.info.append((count,count+len_seq))\n",
        "        count += len_seq\n",
        "\n",
        "      self.padded = np.vstack(self.padded)\n",
        "\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        # en `scores` iremos guardando la perplejidad de cada secuencia\n",
        "        scores = []\n",
        "\n",
        "        predictions = self.model.predict(self.padded,verbose=0)\n",
        "\n",
        "        # para cada secuencia de validación\n",
        "        for start,end in self.info:\n",
        "\n",
        "          # en `probs` iremos guardando las probabilidades de los términos target\n",
        "          probs = [predictions[idx_seq,idx_vocab] for idx_seq, idx_vocab in zip(range(start,end),self.target[start:end])]\n",
        "\n",
        "          # calculamos la perplejidad por medio de logaritmos\n",
        "          scores.append(np.exp(-np.sum(np.log(probs))/(end-start)))\n",
        "\n",
        "        # promediamos todos los scores e imprimimos el valor promedio\n",
        "        print(f'\\n log of mean perplexity: {np.log(np.mean(scores))} \\n')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:18.636909Z",
          "iopub.execute_input": "2024-04-21T14:13:18.637188Z",
          "iopub.status.idle": "2024-04-21T14:13:18.648067Z",
          "shell.execute_reply.started": "2024-04-21T14:13:18.637165Z",
          "shell.execute_reply": "2024-04-21T14:13:18.647045Z"
        },
        "trusted": true,
        "id": "FSsUDOEoQxUN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 1: LSTM\n"
      ],
      "metadata": {
        "id": "J1YpwKENQxUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Elegimos un primer modelo\n",
        "model_1 = Sequential()\n",
        "model_1.add(Embedding(input_dim=vocab_size+1, output_dim=5, input_shape=(max_context_size,)))\n",
        "model_1.add(LSTM(64, return_sequences=True)) # Por ahí cambiar a LSTM 32\n",
        "model_1.add(Dropout(0.2))\n",
        "model_1.add(LSTM(64)) # por ahí también cambiar a 32 también\n",
        "model_1.add(Dense(32, activation='relu'))\n",
        "model_1.add(Dense(vocab_size+1, activation='softmax'))\n",
        "\n",
        "# Compilamos y realizamos un summary\n",
        "model_1.compile(loss=SparseCategoricalCrossentropy(), optimizer='adam',\n",
        "                 metrics=['accuracy'])\n",
        "model_1.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:17.261386Z",
          "iopub.execute_input": "2024-04-21T14:13:17.261747Z",
          "iopub.status.idle": "2024-04-21T14:13:18.633489Z",
          "shell.execute_reply.started": "2024-04-21T14:13:17.261711Z",
          "shell.execute_reply": "2024-04-21T14:13:18.632482Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQWZ_VWEQxUN",
        "outputId": "de3c9ddc-2893-4f76-bd25-b9bb16526204"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 6, 5)              56245     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 6, 64)             17920     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 6, 64)             0         \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 64)                33024     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 11249)             371217    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 480486 (1.83 MB)\n",
            "Trainable params: 480486 (1.83 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fiteamos el modelo\n",
        "hist = model_1.fit(X, y, epochs=5,\n",
        "                 callbacks=[PplCallback(tokenized_sentences_val)],\n",
        "                 batch_size=32)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T14:13:45.059837Z",
          "iopub.execute_input": "2024-04-21T14:13:45.060203Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgiAtibmQxUO",
        "outputId": "b61bd314-48b7-4829-db50-e0e605f2e595"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "2136/2140 [============================>.] - ETA: 0s - loss: 7.7876 - accuracy: 0.0489\n",
            " log of mean perplexity: 8.860346617794518 \n",
            "\n",
            "2140/2140 [==============================] - 34s 14ms/step - loss: 7.7870 - accuracy: 0.0489\n",
            "Epoch 2/5\n",
            "2139/2140 [============================>.] - ETA: 0s - loss: 7.3944 - accuracy: 0.0489\n",
            " log of mean perplexity: 10.019685878068739 \n",
            "\n",
            "2140/2140 [==============================] - 24s 11ms/step - loss: 7.3944 - accuracy: 0.0490\n",
            "Epoch 3/5\n",
            "2140/2140 [==============================] - ETA: 0s - loss: 7.2125 - accuracy: 0.0554\n",
            " log of mean perplexity: 10.917174385256844 \n",
            "\n",
            "2140/2140 [==============================] - 27s 13ms/step - loss: 7.2125 - accuracy: 0.0554\n",
            "Epoch 4/5\n",
            "2137/2140 [============================>.] - ETA: 0s - loss: 6.9715 - accuracy: 0.0600\n",
            " log of mean perplexity: 12.438051468339719 \n",
            "\n",
            "2140/2140 [==============================] - 24s 11ms/step - loss: 6.9716 - accuracy: 0.0599\n",
            "Epoch 5/5\n",
            "2139/2140 [============================>.] - ETA: 0s - loss: 6.7693 - accuracy: 0.0631\n",
            " log of mean perplexity: 14.395304791775006 \n",
            "\n",
            "2140/2140 [==============================] - 23s 11ms/step - loss: 6.7692 - accuracy: 0.0631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 2: Conv1D y Bidirectional LSTM\n",
        "- Al no poder disminuir la perplexity, vamos a usar un modelo más complejo."
      ],
      "metadata": {
        "id": "8YcccSJuZxIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Segundo modelo\n",
        "model_2 = Sequential()\n",
        "model_2.add(Embedding(input_dim=vocab_size+1, output_dim=5, input_shape=(max_context_size,)))\n",
        "\n",
        "# Capas convolucionales:\n",
        "model_2.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "model_2.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Capas recurrentes bidireccionales (BRNN):\n",
        "model_2.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model_2.add(Dropout(0.2))\n",
        "\n",
        "# Capa LSTM adicional:\n",
        "model_2.add(Bidirectional(LSTM(64)))\n",
        "\n",
        "# Capa densa con ReLU:\n",
        "model_2.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Predicción de clasificación con softmax:\n",
        "model_2.add(Dense(vocab_size+1, activation='softmax'))\n",
        "\n",
        "# Compilamos\n",
        "model_2.compile(loss=SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6GM_nlyQxUP",
        "outputId": "849faf17-3228-494c-a5c8-9a131bf1e799"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 6, 5)              56245     \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 4, 32)             512       \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 2, 32)             0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 2, 128)            49664     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 2, 128)            0         \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 128)               98816     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 32)                4128      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 11249)             371217    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 580582 (2.21 MB)\n",
            "Trainable params: 580582 (2.21 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el segundo modelo\n",
        "hist_2 = model_2.fit(X, y, epochs=5, callbacks=[PplCallback(tokenized_sentences_val)], batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrdFqXDQQxUP",
        "outputId": "9c3f02eb-b3c3-436a-f496-b69cfd9b0ce5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "2140/2140 [==============================] - ETA: 0s - loss: 7.7629 - accuracy: 0.0488\n",
            " log of mean perplexity: 8.66168976516364 \n",
            "\n",
            "2140/2140 [==============================] - 47s 18ms/step - loss: 7.7629 - accuracy: 0.0488\n",
            "Epoch 2/5\n",
            "2139/2140 [============================>.] - ETA: 0s - loss: 7.3659 - accuracy: 0.0491\n",
            " log of mean perplexity: 8.939096622679296 \n",
            "\n",
            "2140/2140 [==============================] - 37s 17ms/step - loss: 7.3659 - accuracy: 0.0491\n",
            "Epoch 3/5\n",
            "2137/2140 [============================>.] - ETA: 0s - loss: 7.1890 - accuracy: 0.0527\n",
            " log of mean perplexity: 9.601253376294887 \n",
            "\n",
            "2140/2140 [==============================] - 33s 15ms/step - loss: 7.1883 - accuracy: 0.0528\n",
            "Epoch 4/5\n",
            "2139/2140 [============================>.] - ETA: 0s - loss: 6.9780 - accuracy: 0.0584\n",
            " log of mean perplexity: 9.915274684658115 \n",
            "\n",
            "2140/2140 [==============================] - 33s 15ms/step - loss: 6.9779 - accuracy: 0.0584\n",
            "Epoch 5/5\n",
            "2138/2140 [============================>.] - ETA: 0s - loss: 6.7940 - accuracy: 0.0634\n",
            " log of mean perplexity: 11.372308218972051 \n",
            "\n",
            "2140/2140 [==============================] - 41s 19ms/step - loss: 6.7941 - accuracy: 0.0634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 3: Bidirectional LSTM stack\n",
        "- Al no bajar la perplejidad, decidimos realizar un modelo más complejo."
      ],
      "metadata": {
        "id": "rOTHKXuwbWIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizamos un tercer modelo\n",
        "model_3 = Sequential()\n",
        "model_3.add(Embedding(input_dim=vocab_size+1, output_dim=5, input_shape=(max_context_size,)))\n",
        "\n",
        "# Capas convolucionales:\n",
        "model_3.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "model_3.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "# Capas recurrentes bidireccionales (BRNN):\n",
        "model_3.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model_3.add(Dropout(0.2))\n",
        "model_3.add(Bidirectional(LSTM(128)))\n",
        "\n",
        "# Capa densa con ReLU:\n",
        "model_3.add(Dense(64, activation='relu'))\n",
        "model_3.add(Dense(vocab_size+1, activation='softmax'))\n",
        "\n",
        "# Compilamos\n",
        "model_3.compile(loss=SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_GgNoJ3beaf",
        "outputId": "48f64e7d-573e-48c8-e3da-093bd5788291"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 6, 5)              56245     \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 4, 32)             512       \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPoolin  (None, 2, 32)             0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirecti  (None, 2, 128)            49664     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 2, 128)            0         \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirecti  (None, 256)               263168    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 11249)             731185    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1117222 (4.26 MB)\n",
            "Trainable params: 1117222 (4.26 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el tercer modelo\n",
        "hist_3 = model_3.fit(X, y, epochs=5, callbacks=[PplCallback(tokenized_sentences_val)], batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsK7emjxbcJe",
        "outputId": "5a1962a8-09e2-4ca3-b3db-b2428ce59909"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "2139/2140 [============================>.] - ETA: 0s - loss: 7.7487 - accuracy: 0.0490\n",
            " log of mean perplexity: 8.423373927211196 \n",
            "\n",
            "2140/2140 [==============================] - 38s 18ms/step - loss: 7.7490 - accuracy: 0.0490\n",
            "Epoch 2/5\n",
            "2140/2140 [==============================] - ETA: 0s - loss: 7.3417 - accuracy: 0.0501\n",
            " log of mean perplexity: 8.95627649663855 \n",
            "\n",
            "2140/2140 [==============================] - 33s 15ms/step - loss: 7.3417 - accuracy: 0.0501\n",
            "Epoch 3/5\n",
            "2138/2140 [============================>.] - ETA: 0s - loss: 7.1182 - accuracy: 0.0538\n",
            " log of mean perplexity: 9.20982114105152 \n",
            "\n",
            "2140/2140 [==============================] - 34s 16ms/step - loss: 7.1177 - accuracy: 0.0538\n",
            "Epoch 4/5\n",
            "2137/2140 [============================>.] - ETA: 0s - loss: 6.9115 - accuracy: 0.0582\n",
            " log of mean perplexity: 10.07518649847256 \n",
            "\n",
            "2140/2140 [==============================] - 31s 15ms/step - loss: 6.9121 - accuracy: 0.0581\n",
            "Epoch 5/5\n",
            "2136/2140 [============================>.] - ETA: 0s - loss: 6.7311 - accuracy: 0.0614\n",
            " log of mean perplexity: 10.897553924110785 \n",
            "\n",
            "2140/2140 [==============================] - 34s 16ms/step - loss: 6.7317 - accuracy: 0.0613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 4: Conv1D stack, Bidirectional LSTM y GRU\n",
        "- Añadimos mas complejidad a la arquitectura."
      ],
      "metadata": {
        "id": "nnM7JhARdmhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizamos un cuarto modelo\n",
        "model_4 = Sequential()\n",
        "model_4.add(Embedding(input_dim=vocab_size+1, output_dim=5, input_shape=(max_context_size,)))\n",
        "\n",
        "# Capas convolucionales:\n",
        "model_4.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "model_4.add(MaxPooling1D(pool_size=2))\n",
        "model_4.add(Dropout(0.2))\n",
        "model_4.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
        "model_4.add(Dropout(0.2))\n",
        "\n",
        "# Capas recurrentes bidireccionales (BRNN):\n",
        "model_4.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model_4.add(Bidirectional(GRU(128)))\n",
        "\n",
        "# Capa densa con ReLU:\n",
        "model_4.add(Dense(64, activation='relu'))\n",
        "model_4.add(Dense(vocab_size+1, activation='softmax'))\n",
        "\n",
        "# Compilamos\n",
        "model_4.compile(loss=SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_4.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kO4gO-80diq-",
        "outputId": "936c1a08-6dd0-41e0-d41f-3fa1dad64835"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_17 (Embedding)    (None, 6, 5)              56245     \n",
            "                                                                 \n",
            " conv1d_19 (Conv1D)          (None, 4, 32)             512       \n",
            "                                                                 \n",
            " max_pooling1d_15 (MaxPooli  (None, 2, 32)             0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 2, 32)             0         \n",
            "                                                                 \n",
            " conv1d_20 (Conv1D)          (None, 1, 64)             4160      \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 1, 64)             0         \n",
            "                                                                 \n",
            " bidirectional_17 (Bidirect  (None, 1, 128)            66048     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_18 (Bidirect  (None, 256)               198144    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 11249)             731185    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1072742 (4.09 MB)\n",
            "Trainable params: 1072742 (4.09 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el cuarto modelo\n",
        "hist_4 = model_4.fit(X, y, epochs=5, callbacks=[PplCallback(tokenized_sentences_val)], batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FdLHx_XdimU",
        "outputId": "926d7746-64de-4fb8-b257-df2dda3c1638"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "2140/2140 [==============================] - ETA: 0s - loss: 7.7635 - accuracy: 0.0489\n",
            " log of mean perplexity: 8.425652962234297 \n",
            "\n",
            "2140/2140 [==============================] - 49s 19ms/step - loss: 7.7635 - accuracy: 0.0489\n",
            "Epoch 2/5\n",
            "2140/2140 [==============================] - ETA: 0s - loss: 7.3804 - accuracy: 0.0491\n",
            " log of mean perplexity: 9.178695753373011 \n",
            "\n",
            "2140/2140 [==============================] - 38s 18ms/step - loss: 7.3804 - accuracy: 0.0491\n",
            "Epoch 3/5\n",
            "2138/2140 [============================>.] - ETA: 0s - loss: 7.2279 - accuracy: 0.0518\n",
            " log of mean perplexity: 9.245586859792926 \n",
            "\n",
            "2140/2140 [==============================] - 36s 17ms/step - loss: 7.2279 - accuracy: 0.0518\n",
            "Epoch 4/5\n",
            "2139/2140 [============================>.] - ETA: 0s - loss: 7.0807 - accuracy: 0.0543\n",
            " log of mean perplexity: 9.672129547159498 \n",
            "\n",
            "2140/2140 [==============================] - 32s 15ms/step - loss: 7.0808 - accuracy: 0.0543\n",
            "Epoch 5/5\n",
            "2140/2140 [==============================] - ETA: 0s - loss: 6.9512 - accuracy: 0.0573\n",
            " log of mean perplexity: 10.056071968948636 \n",
            "\n",
            "2140/2140 [==============================] - 34s 16ms/step - loss: 6.9512 - accuracy: 0.0573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 5: Aumento de LSTM con GRU\n",
        "- Aumentamos la complejidad de la arquitectura."
      ],
      "metadata": {
        "id": "BiY7kN2wjgSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizamos un quitno modelo\n",
        "model_5 = Sequential()\n",
        "model_5.add(Embedding(input_dim=vocab_size+1, output_dim=5, input_shape=(max_context_size,)))\n",
        "\n",
        "# Capas convolucionales:\n",
        "model_5.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "model_5.add(MaxPooling1D(pool_size=2))\n",
        "model_5.add(Dropout(0.2))\n",
        "model_5.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
        "model_5.add(Dropout(0.2))\n",
        "\n",
        "# Capas recurrentes bidireccionales (BRNN):\n",
        "model_5.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model_5.add(Bidirectional(GRU(128)))\n",
        "model_5.add(Reshape((256, 1)))\n",
        "model_5.add(Bidirectional(LSTM(32)))\n",
        "\n",
        "# Capa densa con ReLU:\n",
        "model_5.add(Dense(64, activation='relu'))\n",
        "model_5.add(Dense(vocab_size+1, activation='softmax'))\n",
        "\n",
        "# Compilamos\n",
        "model_5.compile(loss=SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_5.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qi2mudldiiU",
        "outputId": "4c475786-d7bb-4e81-9385-d2613b292938"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_24 (Embedding)    (None, 6, 5)              56245     \n",
            "                                                                 \n",
            " conv1d_35 (Conv1D)          (None, 4, 32)             512       \n",
            "                                                                 \n",
            " max_pooling1d_23 (MaxPooli  (None, 2, 32)             0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 2, 32)             0         \n",
            "                                                                 \n",
            " conv1d_36 (Conv1D)          (None, 1, 64)             4160      \n",
            "                                                                 \n",
            " dropout_29 (Dropout)        (None, 1, 64)             0         \n",
            "                                                                 \n",
            " bidirectional_42 (Bidirect  (None, 1, 128)            66048     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_43 (Bidirect  (None, 256)               198144    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " reshape_2 (Reshape)         (None, 256, 1)            0         \n",
            "                                                                 \n",
            " bidirectional_44 (Bidirect  (None, 64)                8704      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 11249)             731185    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1069158 (4.08 MB)\n",
            "Trainable params: 1069158 (4.08 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el quinto modelo\n",
        "hist_5 = model_5.fit(X, y, epochs=5, callbacks=[PplCallback(tokenized_sentences_val)], batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-fKyfKNneRA",
        "outputId": "04ec096d-f46b-4729-cd0e-ad6082d1e14b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "2140/2140 [==============================] - ETA: 0s - loss: 7.7744 - accuracy: 0.0486\n",
            " log of mean perplexity: 8.742335505984176 \n",
            "\n",
            "2140/2140 [==============================] - 90s 36ms/step - loss: 7.7744 - accuracy: 0.0486\n",
            "Epoch 2/5\n",
            "2140/2140 [==============================] - ETA: 0s - loss: 7.3584 - accuracy: 0.0491\n",
            " log of mean perplexity: 8.855352119742001 \n",
            "\n",
            "2140/2140 [==============================] - 74s 34ms/step - loss: 7.3584 - accuracy: 0.0491\n",
            "Epoch 3/5\n",
            "2140/2140 [==============================] - ETA: 0s - loss: 7.1943 - accuracy: 0.0519\n",
            " log of mean perplexity: 9.470258420491731 \n",
            "\n",
            "2140/2140 [==============================] - 73s 34ms/step - loss: 7.1943 - accuracy: 0.0519\n",
            "Epoch 4/5\n",
            "2139/2140 [============================>.] - ETA: 0s - loss: 7.0639 - accuracy: 0.0532\n",
            " log of mean perplexity: 10.023485886482852 \n",
            "\n",
            "2140/2140 [==============================] - 73s 34ms/step - loss: 7.0638 - accuracy: 0.0532\n",
            "Epoch 5/5\n",
            "2140/2140 [==============================] - ETA: 0s - loss: 6.9432 - accuracy: 0.0568\n",
            " log of mean perplexity: 11.228882728461413 \n",
            "\n",
            "2140/2140 [==============================] - 69s 32ms/step - loss: 6.9432 - accuracy: 0.0568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 6: Aumento de neuronas\n",
        "- Elegimos el model_4 que tuvo la mejor perplexity y probamos un cambio de numero de neuronas en búsqueda de bajar la perplexity."
      ],
      "metadata": {
        "id": "ZOnnElaPpJoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizamos un cuarto modelo\n",
        "model_6 = Sequential()\n",
        "model_6.add(Embedding(input_dim=vocab_size+1, output_dim=5, input_shape=(max_context_size,)))\n",
        "\n",
        "# Capas convolucionales:\n",
        "model_6.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model_6.add(MaxPooling1D(pool_size=2))\n",
        "model_6.add(Dropout(0.2))\n",
        "model_6.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
        "model_6.add(Dropout(0.2))\n",
        "\n",
        "# Capas recurrentes bidireccionales (BRNN):\n",
        "model_6.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model_6.add(Bidirectional(GRU(256)))\n",
        "\n",
        "# Capa densa con ReLU:\n",
        "model_6.add(Dense(128, activation='relu'))\n",
        "model_6.add(Dense(vocab_size+1, activation='softmax'))\n",
        "\n",
        "# Compilamos\n",
        "model_6.compile(loss=SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_6.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1U4zxIPneNf",
        "outputId": "fa9f71f2-dde6-4731-a997-fa57d66b7da0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_26 (Embedding)    (None, 6, 5)              56245     \n",
            "                                                                 \n",
            " conv1d_39 (Conv1D)          (None, 4, 64)             1024      \n",
            "                                                                 \n",
            " max_pooling1d_25 (MaxPooli  (None, 2, 64)             0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " dropout_32 (Dropout)        (None, 2, 64)             0         \n",
            "                                                                 \n",
            " conv1d_40 (Conv1D)          (None, 1, 128)            16512     \n",
            "                                                                 \n",
            " dropout_33 (Dropout)        (None, 1, 128)            0         \n",
            "                                                                 \n",
            " bidirectional_47 (Bidirect  (None, 1, 256)            263168    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_48 (Bidirect  (None, 512)               789504    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 128)               65664     \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 11249)             1451121   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2643238 (10.08 MB)\n",
            "Trainable params: 2643238 (10.08 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el sexto modelo\n",
        "hist_6 = model_6.fit(X, y, epochs=5, callbacks=[PplCallback(tokenized_sentences_val)], batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYdhnrUqneJY",
        "outputId": "5d9a48ef-19c2-408d-a233-0e2f40f82b69"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "2140/2140 [==============================] - ETA: 0s - loss: 7.7595 - accuracy: 0.0486\n",
            " log of mean perplexity: 8.533316446277947 \n",
            "\n",
            "2140/2140 [==============================] - 49s 19ms/step - loss: 7.7595 - accuracy: 0.0486\n",
            "Epoch 2/5\n",
            "2138/2140 [============================>.] - ETA: 0s - loss: 7.3418 - accuracy: 0.0504\n",
            " log of mean perplexity: 9.04059275900507 \n",
            "\n",
            "2140/2140 [==============================] - 39s 18ms/step - loss: 7.3417 - accuracy: 0.0504\n",
            "Epoch 3/5\n",
            "2140/2140 [==============================] - ETA: 0s - loss: 7.1417 - accuracy: 0.0535\n",
            " log of mean perplexity: 9.471852932054372 \n",
            "\n",
            "2140/2140 [==============================] - 34s 16ms/step - loss: 7.1417 - accuracy: 0.0535\n",
            "Epoch 4/5\n",
            "2139/2140 [============================>.] - ETA: 0s - loss: 6.9660 - accuracy: 0.0565\n",
            " log of mean perplexity: 9.958456520526207 \n",
            "\n",
            "2140/2140 [==============================] - 35s 16ms/step - loss: 6.9661 - accuracy: 0.0565\n",
            "Epoch 5/5\n",
            "2139/2140 [============================>.] - ETA: 0s - loss: 6.8119 - accuracy: 0.0588\n",
            " log of mean perplexity: 10.628642654157646 \n",
            "\n",
            "2140/2140 [==============================] - 31s 15ms/step - loss: 6.8120 - accuracy: 0.0588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 7: Cambio de size del contexto máximo\n",
        "- Como no mejoró el valor de perplexidad, vamos a modificar el tamaño del contexto máximo."
      ],
      "metadata": {
        "id": "fSi5wPmKrKBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Elegimos el máximo valor de la distribución\n",
        "max_context_size = 12"
      ],
      "metadata": {
        "id": "ixxSRcsLneFe"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el tokenizador\n",
        "tok = Tokenizer()\n",
        "\n",
        "# El tokenizer \"aprende\" las palabras que se usaran\n",
        "# Se construye (fit) una vez por proyecto, se aplica N veces (tal cual un encoder)\n",
        "# El token 0 es reservado y no es asignado. Se utiliza para designar a palabras\n",
        "# fuera del vocabulario aprendido\n",
        "tok.fit_on_texts(segmented_sentences)\n",
        "\n",
        "# Convertimos las palabras a números\n",
        "# entran palabras -> salen números\n",
        "tokenized_sentences = tok.texts_to_sequences(segmented_sentences)"
      ],
      "metadata": {
        "id": "b8cJ1VoT3-sg"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizamos un separar las oraciones que tienen el tamaño más grande que el máximo tamaño de contexto\n",
        "tok_sent = []\n",
        "\n",
        "for sent in tokenized_sentences_train:\n",
        "\n",
        "  # si la secuencia tiene más términos que el tamaño de contexto máximo,\n",
        "  # armo varias sub-secuencias de tamaño máximo\n",
        "  if len(sent) > (max_context_size+1):\n",
        "    extra = len(sent)-(max_context_size+1) + 1\n",
        "    for i in range(extra):\n",
        "      tok_sent.append(sent[i:i+max_context_size+1])\n",
        "  else: # si la secuencia tiene menos términos el tamaño de contexto máximo, dejo la secuencia como está\n",
        "    tok_sent.append(sent)\n",
        "\n",
        "# Realizamos un data augmentation de estas oraciones\n",
        "tok_sent_augm = []\n",
        "\n",
        "for sent in tok_sent:\n",
        "\n",
        "  # generamos todas las sub-secuencias\n",
        "  subseq = [sent[:i+2] for i in range(len(sent)-1)]\n",
        "  # en esta línea paddeamos al tamaño de contexto máximo\n",
        "  tok_sent_augm.append(pad_sequences(subseq, maxlen=max_context_size+1, padding='pre'))\n",
        "\n",
        "# Realizamos una concatenación de las secuencias y vemo su shape\n",
        "train_seqs = np.concatenate(tok_sent_augm, axis=0)\n",
        "print(f\"Shape del dataset segmentado y aumentado: {train_seqs.shape}\")\n",
        "\n",
        "# Generamos el X y el y\n",
        "X = train_seqs[:,:-1]\n",
        "y = train_seqs[:,-1]\n",
        "\n",
        "# Vemos cuantas palabras tiene el vocabulario\n",
        "# Cantidad de palabras en el vocabulario\n",
        "vocab_size = len(tok.word_counts)\n",
        "print(f'El tamaño del vocabulario es ahora: {vocab_size}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URdbKEAwrHqd",
        "outputId": "0263185d-c283-4735-8085-443fb7f5e992"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape del dataset segmentado y aumentado: (54045, 13)\n",
            "El tamaño del vocabulario es ahora: 23101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizamos un séptimo modelo de la base del model_4\n",
        "model_7 = Sequential()\n",
        "model_7.add(Embedding(input_dim=vocab_size+1, output_dim=5, input_shape=(max_context_size,)))\n",
        "\n",
        "# Capas convolucionales:\n",
        "model_7.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "model_7.add(MaxPooling1D(pool_size=2))\n",
        "model_7.add(Dropout(0.2))\n",
        "model_7.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
        "model_7.add(Dropout(0.2))\n",
        "\n",
        "# Capas recurrentes bidireccionales (BRNN):\n",
        "model_7.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model_7.add(Bidirectional(GRU(128)))\n",
        "\n",
        "# Capa densa con ReLU:\n",
        "model_7.add(Dense(64, activation='relu'))\n",
        "model_7.add(Dense(vocab_size+1, activation='softmax'))\n",
        "\n",
        "# Compilamos\n",
        "model_7.compile(loss=SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_7.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfG__v7zrHhi",
        "outputId": "e7ec6bc4-775d-4faa-beb7-5d584be10144"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_27 (Embedding)    (None, 12, 5)             56245     \n",
            "                                                                 \n",
            " conv1d_41 (Conv1D)          (None, 10, 32)            512       \n",
            "                                                                 \n",
            " max_pooling1d_26 (MaxPooli  (None, 5, 32)             0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " dropout_34 (Dropout)        (None, 5, 32)             0         \n",
            "                                                                 \n",
            " conv1d_42 (Conv1D)          (None, 4, 64)             4160      \n",
            "                                                                 \n",
            " dropout_35 (Dropout)        (None, 4, 64)             0         \n",
            "                                                                 \n",
            " bidirectional_49 (Bidirect  (None, 4, 128)            66048     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_50 (Bidirect  (None, 256)               198144    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 11249)             731185    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1072742 (4.09 MB)\n",
            "Trainable params: 1072742 (4.09 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el séptimo modelo\n",
        "hist_7 = model_7.fit(X, y, epochs=5, callbacks=[PplCallback(tokenized_sentences_val)], batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boHYofRZrHY3",
        "outputId": "b85ec8af-f0d9-42a4-d037-2a8a08effd1b"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1689/1689 [==============================] - ETA: 0s - loss: 7.9789 - accuracy: 0.0433\n",
            " log of mean perplexity: 8.495170524984351 \n",
            "\n",
            "1689/1689 [==============================] - 46s 20ms/step - loss: 7.9789 - accuracy: 0.0433\n",
            "Epoch 2/5\n",
            "1689/1689 [==============================] - ETA: 0s - loss: 7.5617 - accuracy: 0.0437\n",
            " log of mean perplexity: 8.769002798477448 \n",
            "\n",
            "1689/1689 [==============================] - 28s 17ms/step - loss: 7.5617 - accuracy: 0.0437\n",
            "Epoch 3/5\n",
            "1688/1689 [============================>.] - ETA: 0s - loss: 7.3880 - accuracy: 0.0478\n",
            " log of mean perplexity: 9.32348634777488 \n",
            "\n",
            "1689/1689 [==============================] - 29s 17ms/step - loss: 7.3886 - accuracy: 0.0478\n",
            "Epoch 4/5\n",
            "1688/1689 [============================>.] - ETA: 0s - loss: 7.2078 - accuracy: 0.0510\n",
            " log of mean perplexity: 9.63043988931439 \n",
            "\n",
            "1689/1689 [==============================] - 30s 18ms/step - loss: 7.2078 - accuracy: 0.0510\n",
            "Epoch 5/5\n",
            "1689/1689 [==============================] - ETA: 0s - loss: 7.0653 - accuracy: 0.0526\n",
            " log of mean perplexity: 9.873036639152224 \n",
            "\n",
            "1689/1689 [==============================] - 27s 16ms/step - loss: 7.0653 - accuracy: 0.0526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo 8: Cambio del tamaño del corpus\n",
        "- Como ninguno de estos 7 modelos mejoró, vamos a aumentar el tamaño del corpus siempre y cuando tengamos memoria suficiente para poder entrenar."
      ],
      "metadata": {
        "id": "2KfhbxYvuG1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dejamos la columna que nos importa y calculamos la dimensión\n",
        "df = raw_df['headline_text']\n",
        "print(f\"El numero de filas (headlines) que tiene el corpus es: {len(df)}\")\n",
        "\n",
        "# Por una cuestión de tiempo de procesamiento, tomamos una parte del corpus\n",
        "tenperc_text = len(df)/5\n",
        "df = df.iloc[0:int(tenperc_text)]\n",
        "print(f'La nueva longitud del texto es: {len(df)} filas')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXq-VHd2rHPo",
        "outputId": "61ffbfc0-3e73-4331-8e56-8cd35fbe65a6"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El numero de filas (headlines) que tiene el corpus es: 1244184\n",
            "La nueva longitud del texto es: 248836 filas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cada verso lo guardamos en una lista\n",
        "text = list(df)\n",
        "print(f'La longitud del texto es: {len(text)}')\n",
        "print(f'Vemos los primero headlines de la lista: {text[:10]}')\n",
        "\n",
        "# Segmentamos el texto con la utilidad de Keras\n",
        "segmented_sentences = [text_to_word_sequence(sentence) for sentence in text]\n",
        "\n",
        "# Calculamos la longitud de cada secuencia\n",
        "length_sentences = [len(sentence) for sentence in segmented_sentences]\n",
        "\n",
        "# Podemos ver su distribución\n",
        "plt.hist(length_sentences,bins=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "GleMRkM4rHFs",
        "outputId": "ed16880b-1d1d-4d9a-9252-833532403744"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La longitud del texto es: 248836\n",
            "Vemos los primero headlines de la lista: ['aba decides against community broadcasting licence', 'act fire witnesses must be aware of defamation', 'a g calls for infrastructure protection summit', 'air nz staff in aust strike for pay rise', 'air nz strike to affect australian travellers', 'ambitious olsson wins triple jump', 'antic delighted with record breaking barca', 'aussie qualifier stosur wastes four memphis match', 'aust addresses un security council over iraq', 'australia is locked into war timetable opp']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Elegimos el contexto máximo como 12\n",
        "max_context_size = 12"
      ],
      "metadata": {
        "id": "pbkAO9J95mbR"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el tokenizador\n",
        "tok = Tokenizer()\n",
        "\n",
        "# El tokenizer \"aprende\" las palabras que se usaran\n",
        "# Se construye (fit) una vez por proyecto, se aplica N veces (tal cual un encoder)\n",
        "# El token 0 es reservado y no es asignado. Se utiliza para designar a palabras\n",
        "# fuera del vocabulario aprendido\n",
        "tok.fit_on_texts(segmented_sentences)\n",
        "\n",
        "# Convertimos las palabras a números\n",
        "# entran palabras -> salen números\n",
        "tokenized_sentences = tok.texts_to_sequences(segmented_sentences)"
      ],
      "metadata": {
        "id": "Ed5UK-VP30BQ"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Como se mantiene el contexto máximo podemos seguir de la siguiente manera\n",
        "# Realizamos un separar las oraciones que tienen el tamaño más grande que el máximo tamaño de contexto\n",
        "tok_sent = []\n",
        "\n",
        "for sent in tokenized_sentences_train:\n",
        "\n",
        "  # si la secuencia tiene más términos que el tamaño de contexto máximo,\n",
        "  # armo varias sub-secuencias de tamaño máximo\n",
        "  if len(sent) > (max_context_size+1):\n",
        "    extra = len(sent)-(max_context_size+1) + 1\n",
        "    for i in range(extra):\n",
        "      tok_sent.append(sent[i:i+max_context_size+1])\n",
        "  else: # si la secuencia tiene menos términos el tamaño de contexto máximo, dejo la secuencia como está\n",
        "    tok_sent.append(sent)\n",
        "\n",
        "# Realizamos un data augmentation de estas oraciones\n",
        "tok_sent_augm = []\n",
        "\n",
        "for sent in tok_sent:\n",
        "\n",
        "  # generamos todas las sub-secuencias\n",
        "  subseq = [sent[:i+2] for i in range(len(sent)-1)]\n",
        "  # en esta línea paddeamos al tamaño de contexto máximo\n",
        "  tok_sent_augm.append(pad_sequences(subseq, maxlen=max_context_size+1, padding='pre'))\n",
        "\n",
        "# Realizamos una concatenación de las secuencias y vemo su shape\n",
        "train_seqs = np.concatenate(tok_sent_augm, axis=0)\n",
        "print(f\"Shape del dataset segmentado y aumentado: {train_seqs.shape}\")\n",
        "\n",
        "# Generamos el X y el y\n",
        "X = train_seqs[:,:-1]\n",
        "y = train_seqs[:,-1]\n",
        "\n",
        "# Vemos cuantas palabras tiene el vocabulario\n",
        "# Cantidad de palabras en el vocabulario\n",
        "vocab_size = len(tok.word_counts)\n",
        "print(f'El tamaño del vocabulario es ahora: {vocab_size}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTntl1EZu8CP",
        "outputId": "9a33547d-b8ad-44d6-f3bb-f99075ba48ae"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape del dataset segmentado y aumentado: (54045, 13)\n",
            "El tamaño del vocabulario es ahora: 38997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizamos un séptimo modelo de la base del model_4\n",
        "model_8 = Sequential()\n",
        "model_8.add(Embedding(input_dim=vocab_size+1, output_dim=5, input_shape=(max_context_size,)))\n",
        "\n",
        "# Capas convolucionales:\n",
        "model_8.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
        "model_8.add(MaxPooling1D(pool_size=2))\n",
        "model_8.add(Dropout(0.2))\n",
        "model_8.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
        "model_8.add(Dropout(0.2))\n",
        "\n",
        "# Capas recurrentes bidireccionales (BRNN):\n",
        "model_8.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model_8.add(Bidirectional(GRU(128)))\n",
        "\n",
        "# Capa densa con ReLU:\n",
        "model_8.add(Dense(64, activation='relu'))\n",
        "model_8.add(Dense(vocab_size+1, activation='softmax'))\n",
        "\n",
        "# Compilamos\n",
        "model_8.compile(loss=SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_8.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEU1ZEW-u7_i",
        "outputId": "b72216e9-7e25-45a9-98ec-f15f8cd25f1f"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_31 (Embedding)    (None, 12, 5)             194990    \n",
            "                                                                 \n",
            " conv1d_49 (Conv1D)          (None, 10, 32)            512       \n",
            "                                                                 \n",
            " max_pooling1d_30 (MaxPooli  (None, 5, 32)             0         \n",
            " ng1D)                                                           \n",
            "                                                                 \n",
            " dropout_42 (Dropout)        (None, 5, 32)             0         \n",
            "                                                                 \n",
            " conv1d_50 (Conv1D)          (None, 4, 64)             4160      \n",
            "                                                                 \n",
            " dropout_43 (Dropout)        (None, 4, 64)             0         \n",
            "                                                                 \n",
            " bidirectional_57 (Bidirect  (None, 4, 128)            66048     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_58 (Bidirect  (None, 256)               198144    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 38998)             2534870   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3015172 (11.50 MB)\n",
            "Trainable params: 3015172 (11.50 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el octavo modelo\n",
        "hist_8 = model_8.fit(X, y, epochs=5, callbacks=[PplCallback(tokenized_sentences_val)], batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cPTcANsu79L",
        "outputId": "a0e7f1d5-ffc7-46bf-b83a-35ac9f7e4ce7"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1689/1689 [==============================] - ETA: 0s - loss: 8.2396 - accuracy: 0.0419\n",
            " log of mean perplexity: 8.685116231663697 \n",
            "\n",
            "1689/1689 [==============================] - 59s 30ms/step - loss: 8.2396 - accuracy: 0.0419\n",
            "Epoch 2/5\n",
            "1689/1689 [==============================] - ETA: 0s - loss: 7.6159 - accuracy: 0.0434\n",
            " log of mean perplexity: 9.461838942380119 \n",
            "\n",
            "1689/1689 [==============================] - 39s 23ms/step - loss: 7.6159 - accuracy: 0.0434\n",
            "Epoch 3/5\n",
            "1687/1689 [============================>.] - ETA: 0s - loss: 7.4684 - accuracy: 0.0430\n",
            " log of mean perplexity: 9.600930592003133 \n",
            "\n",
            "1689/1689 [==============================] - 39s 23ms/step - loss: 7.4685 - accuracy: 0.0430\n",
            "Epoch 4/5\n",
            "1689/1689 [==============================] - ETA: 0s - loss: 7.3347 - accuracy: 0.0461\n",
            " log of mean perplexity: 9.713339085253834 \n",
            "\n",
            "1689/1689 [==============================] - 43s 26ms/step - loss: 7.3347 - accuracy: 0.0461\n",
            "Epoch 5/5\n",
            "1687/1689 [============================>.] - ETA: 0s - loss: 7.1830 - accuracy: 0.0502\n",
            " log of mean perplexity: 10.565859316720447 \n",
            "\n",
            "1689/1689 [==============================] - 38s 23ms/step - loss: 7.1828 - accuracy: 0.0502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- No se pudo mejorar la perplexity pero vamos a utilzar el mejor modelo, de valor más bajo para predecir."
      ],
      "metadata": {
        "id": "7bvpT-DE9IFc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicción de la útlima palabra\n"
      ],
      "metadata": {
        "id": "z7Gv9giSqHOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN9TdwdNqLb6",
        "outputId": "c7d0a1fd-ff7a-4c87-a386-f833023b676e"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gradio as gr\n",
        "\n",
        "def model_response(human_text):\n",
        "\n",
        "    # Encodeamos\n",
        "    encoded = tok.texts_to_sequences([human_text])[0]\n",
        "    # Si tienen distinto largo\n",
        "    encoded = pad_sequences([encoded], maxlen=max_context_size, padding='pre')\n",
        "\n",
        "    # Predicción softmax\n",
        "    y_hat = model.predict(encoded).argmax(axis=-1)\n",
        "\n",
        "    # Debemos buscar en el vocabulario la palabra\n",
        "    # que corresopnde al indice (y_hat) predicho por le modelo\n",
        "    out_word = ''\n",
        "    for word, index in tok.word_index.items():\n",
        "        if index == y_hat:\n",
        "            out_word = word\n",
        "            break\n",
        "\n",
        "    # Agrego la palabra a la frase predicha\n",
        "    return human_text + ' ' + out_word\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=model_response,\n",
        "    inputs=[\"textbox\"],\n",
        "    outputs=\"text\")\n",
        "\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "Pj_C3Za_neA3",
        "outputId": "af549b2e-0e7a-4f44-cde7-f11c3f15c2e3"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://2a38553a636d58be52.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2a38553a636d58be52.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://2a38553a636d58be52.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generación de secuencias"
      ],
      "metadata": {
        "id": "BNTHRb5yqol0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_seq(model, tokenizer, seed_text, max_length, n_words):\n",
        "    \"\"\"\n",
        "        Exec model sequence prediction\n",
        "\n",
        "        Args:\n",
        "            model (keras): modelo entrenado\n",
        "            tokenizer (keras tokenizer): tonenizer utilizado en el preprocesamiento\n",
        "            seed_text (string): texto de entrada (input_seq)\n",
        "            max_length (int): máxima longitud de la sequencia de entrada\n",
        "            n_words (int): números de palabras a agregar a la sequencia de entrada\n",
        "        returns:\n",
        "            output_text (string): sentencia con las \"n_words\" agregadas\n",
        "    \"\"\"\n",
        "    output_text = seed_text\n",
        "\t# generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "\t\t# Encodeamos\n",
        "        encoded = tokenizer.texts_to_sequences([output_text])[0]\n",
        "\t\t# Si tienen distinto largo\n",
        "        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "\t\t# Predicción softmax\n",
        "        y_hat = model.predict(encoded).argmax(axis=-1)\n",
        "\t\t# Vamos concatenando las predicciones\n",
        "        out_word = ''\n",
        "\n",
        "        # Debemos buscar en el vocabulario la palabra\n",
        "        # que corresopnde al indice (y_hat) predicho por le modelo\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == y_hat:\n",
        "                out_word = word\n",
        "                break\n",
        "\n",
        "\t\t# Agrego las palabras a la frase predicha\n",
        "        output_text += ' ' + out_word\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "0s58HDCCnd81"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Elegimos el primer texto\n",
        "input_text='fire witnesses must'\n",
        "generate_seq(model_7, tok, input_text, max_length=max_context_size, n_words=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lOksDprWqsQq",
        "outputId": "ed5816d9-fa30-4fd2-da39-1505bd48a7ca"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fire witnesses must to as'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Probamos con un nuevo input\n",
        "input_text='infrastructure protection summit'\n",
        "generate_seq(model_7, tok, input_text, max_length=max_context_size, n_words=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "tSsy5H03_M3A",
        "outputId": "adb79d64-2048-49eb-e526-bd4e5f8ab824"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'infrastructure protection summit to govt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beam search"
      ],
      "metadata": {
        "id": "82z7HOb2qykM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# funcionalidades para hacer encoding y decoding\n",
        "\n",
        "def encode(text,max_length=max_context_size):\n",
        "\n",
        "    encoded = tok.texts_to_sequences([text])[0]\n",
        "    encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
        "\n",
        "    return encoded\n",
        "\n",
        "def decode(seq):\n",
        "    return tok.sequences_to_texts([seq])"
      ],
      "metadata": {
        "id": "dy7HPyi_nd5M"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "# función que selecciona candidatos para el beam search\n",
        "def select_candidates(pred,num_beams,vocab_size,history_probs,history_tokens,temp=1):\n",
        "\n",
        "  # colectar todas las probabilidades para la siguiente búsqueda\n",
        "  pred_large = []\n",
        "\n",
        "  for idx,pp in enumerate(pred):\n",
        "    pred_large.extend(np.log(pp+1E-10)+history_probs[idx])\n",
        "\n",
        "  pred_large = np.array(pred_large)\n",
        "\n",
        "  # criterio de selección\n",
        "  # idx_select = np.argsort(pred_large)[::-1][:num_beams] # beam search determinista\n",
        "  idx_select = np.random.choice(np.arange(pred_large.shape[0]), num_beams, p=softmax(pred_large/temp)) # beam search con muestreo\n",
        "\n",
        "  # traducir a índices de token en el vocabulario\n",
        "  new_history_tokens = np.concatenate((np.array(history_tokens)[idx_select//vocab_size],\n",
        "                        np.array([idx_select%vocab_size]).T),\n",
        "                      axis=1)\n",
        "\n",
        "  # devolver el producto de las probabilidades (log) y la secuencia de tokens seleccionados\n",
        "  return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
        "\n",
        "\n",
        "def beam_search(model,num_beams,num_words,input):\n",
        "\n",
        "    # first iteration\n",
        "\n",
        "    # encode\n",
        "    encoded = encode(input)\n",
        "\n",
        "    # first prediction\n",
        "    y_hat = np.squeeze(model.predict(encoded))\n",
        "\n",
        "    # get vocabulary size\n",
        "    vocab_size = y_hat.shape[0]\n",
        "\n",
        "    # initialize history\n",
        "    history_probs = [0]*num_beams\n",
        "    history_tokens = [encoded[0]]*num_beams\n",
        "\n",
        "    # select num_beams candidates\n",
        "    history_probs, history_tokens = select_candidates([y_hat],\n",
        "                                        num_beams,\n",
        "                                        vocab_size,\n",
        "                                        history_probs,\n",
        "                                        history_tokens)\n",
        "\n",
        "    # beam search loop\n",
        "    for i in range(num_words-1):\n",
        "\n",
        "      preds = []\n",
        "\n",
        "      for hist in history_tokens:\n",
        "\n",
        "        # actualizar secuencia de tokens\n",
        "        input_update = np.array([hist[i+1:]]).copy()\n",
        "\n",
        "        # predicción\n",
        "        y_hat = np.squeeze(model.predict(input_update))\n",
        "\n",
        "        preds.append(y_hat)\n",
        "\n",
        "      history_probs, history_tokens = select_candidates(preds,\n",
        "                                                        num_beams,\n",
        "                                                        vocab_size,\n",
        "                                                        history_probs,\n",
        "                                                        history_tokens)\n",
        "\n",
        "    return history_tokens"
      ],
      "metadata": {
        "id": "WtcMdB4EqsOh"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicción con beam search\n",
        "salidas = beam_search(model_7,num_beams=10,num_words=6,input=\"fire witnesses must\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JO5HEr7qsMF",
        "outputId": "01ed5bdb-fd74-4b59-f563-3986289b6b2d"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# veamos las salidas\n",
        "decode(salidas[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvhG44RdqsJW",
        "outputId": "73bd94b8-efb5-427e-9d9c-82f7aa0d04e9"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fire witnesses must tipped of govt still for cuts']"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predicción con beam search\n",
        "salidas = beam_search(model_7,num_beams=10,num_words=6,input=\"infraestructure protection summit\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2VAvHzxqsGl",
        "outputId": "6b502e4f-adca-4b5c-b87c-0dcb84e83c0e"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# veamos las salidas\n",
        "decode(salidas[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0E_8gv9qsET",
        "outputId": "d327098b-e56e-4462-efd5-3a7c1c0a6f46"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['protection summit to medical revamp in from sun']"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusión"
      ],
      "metadata": {
        "id": "uPgkcz9uBMgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Se probaron en total 8 modelos modificando:\n",
        "1. Arquitecturas.\n",
        "2. Tipos de capas.\n",
        "3. Aumento de neuronas.\n",
        "4. Cambio de size del contexto máximo.\n",
        "5. Cambio de tamaño del corpus.\n",
        "- Si bien no se puedo llegar a mejorar la perplexity, tenemos una buena generación de texto.\n"
      ],
      "metadata": {
        "id": "7makaVi4BPpg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCxNEyPXBOJM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}